import { ZodSchema } from 'zod';

/**
 * Represents the result of running `llm.act`. Currently only contains minimum amount of
 * information.
 *
 * If you think more information/fields should be added, please open an issue or a PR on GitHub.
 *
 * @public
 */
export declare class ActResult {
    /**
     * Number of rounds performed.
     *
     * For example, in the following scenario:
     *
     * - User asks the model to add 1234 and 5678.
     * - The model requests to use a calculator tool.
     * - The calculator tool outputs 6912.
     * - The calculator's output is then fed back to the model for a second round of prediction.
     * - The model sees the output and generates a paragraph explaining the result.
     *
     * There are 2 rounds. On the beginning of a round, the callback `onRoundStart` is triggered.
     * On the end of a round, the callback `onRoundEnd` is triggered.
     */
    readonly rounds: number;
    /**
     * Total time taken to run `.act` in seconds. measured from beginning of the `.act` invocation
     * to when the entire operation is finished.
     */
    readonly totalExecutionTimeSeconds: number;
    constructor(
    /**
     * Number of rounds performed.
     *
     * For example, in the following scenario:
     *
     * - User asks the model to add 1234 and 5678.
     * - The model requests to use a calculator tool.
     * - The calculator tool outputs 6912.
     * - The calculator's output is then fed back to the model for a second round of prediction.
     * - The model sees the output and generates a paragraph explaining the result.
     *
     * There are 2 rounds. On the beginning of a round, the callback `onRoundStart` is triggered.
     * On the end of a round, the callback `onRoundEnd` is triggered.
     */
    rounds: number, 
    /**
     * Total time taken to run `.act` in seconds. measured from beginning of the `.act` invocation
     * to when the entire operation is finished.
     */
    totalExecutionTimeSeconds: number);
}

/**
 * Type representing the environment variables that can be set by the user.
 *
 * @public
 */
export declare type AllowableEnvVarKeys = "HSA_OVERRIDE_GFX_VERSION";

/**
 * Allow-list only record of environment variables and their values.
 *
 * @public
 */
export declare type AllowableEnvVars = Partial<Record<AllowableEnvVarKeys, string>>;

/**
 * Base type for the manifest of an artifact.
 *
 * @public
 */
export declare interface ArtifactManifestBase {
    owner: string;
    name: string;
    description: string;
    revision?: number;
}

/**
 * @public
 */
export declare interface BackendNotification {
    title: string;
    description?: string;
    noAutoDismiss?: boolean;
}

/** @public */
export declare interface BaseLoadModelOpts<TLoadModelConfig> {
    /**
     * The identifier to use for the loaded model.
     *
     * By default, the identifier is the same as the path (1st parameter). If the identifier already
     * exists, a number will be attached. This option allows you to specify the identifier to use.
     *
     * However, when the identifier is specified and it is in use, an error will be thrown. If the
     * call is successful, it is guaranteed that the loaded model will have the specified identifier.
     */
    identifier?: string;
    /**
     * The configuration to use when loading the model.
     */
    config?: TLoadModelConfig;
    /**
     * An `AbortSignal` to cancel the model loading. This is useful if you wish to add a functionality
     * to cancel the model loading.
     *
     * Example usage:
     *
     * ```typescript
     * const ac = new AbortController();
     * const model = await client.llm.load({
     *   model: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF",
     *   signal: ac.signal,
     * });
     *
     * // Later, to cancel the model loading
     * ac.abort();
     * ```
     *
     * AbortController/AbortSignal is the standard method for cancelling an asynchronous operation in
     * JavaScript. For more information, visit
     * https://developer.mozilla.org/en-US/docs/Web/API/AbortController
     */
    signal?: AbortSignal;
    /**
     * Idle time to live (TTL) in seconds. If specified, when the model is not used for the specified number
     * of seconds, the model will be automatically unloaded. If the model is used before the TTL, the
     * timer will be reset.
     */
    ttl?: number;
    /**
     * Controls the logging of model loading progress.
     *
     * - If set to `true`, logs progress at the "info" level.
     * - If set to `false`, no logs are emitted. This is the default.
     * - If a specific logging level is desired, it can be provided as a string. Acceptable values are
     *   "debug", "info", "warn", and "error".
     *
     * Logs are directed to the logger specified during the `LMStudioClient` construction.
     *
     * Progress logs will be disabled if an `onProgress` callback is provided.
     *
     * Default value is "info", which logs progress at the "info" level.
     */
    verbose?: boolean | LogLevel;
    /**
     * A function that is called with the progress of the model loading. The function is called with a
     * number between 0 and 1, inclusive, representing the progress of the model loading.
     *
     * If an `onProgress` callback is provided, verbose progress logs will be disabled.
     */
    onProgress?: (progress: number) => void;
}

/**
 * Represents a chat history.
 *
 * @public
 */
export declare class Chat extends MaybeMutable<ChatHistoryData> {
    protected getClassName(): string;
    protected create(data: ChatHistoryData, mutable: boolean): this;
    protected cloneData(data: ChatHistoryData): ChatHistoryData;
    /**
     * Don't use this constructor directly.
     *
     * - To create an empty chat history, use `Chat.empty()`.
     * - To create a chat history with existing data, use `Chat.from()`.
     */
    protected constructor(data: ChatHistoryData, mutable: boolean);
    /**
     * Creates an empty mutable chat history.
     */
    static empty(): Chat;
    /**
     * Quickly create a mutable chat history with something that can be converted to a chat history.
     *
     * The created chat history will be a mutable copy of the input.
     *
     * @example
     * ```ts
     * const history = Chat.from([
     *   { role: "user", content: "Hello" },
     *   { role: "assistant", content: "Hi!" },
     *   { role: "user", content: "What is your name?" },
     * ]);
     * ```
     */
    static from(initializer: ChatLike): Chat;
    /**
     * Append a text message to the history.
     */
    append(role: ChatMessageRoleData, content: string, opts?: ChatAppendOpts): void;
    /**
     * Append a message to the history.
     */
    append(message: ChatMessageLike): void;
    /**
     * Make a copy of this history and append a text message to the copy. Return the copy.
     */
    withAppended(role: ChatMessageRoleData, content: string, opts?: ChatAppendOpts): Chat;
    /**
     * Make a copy of this history and append a message to the copy. Return the copy.
     */
    withAppended(message: ChatMessageLike): Chat;
    /**
     * Get the number of messages in the history.
     */
    getLength(): number;
    /**
     * Get the number of messages in the history.
     */
    get length(): number;
    /**
     * Remove the last message from the history. If the history is empty, this method will throw.
     */
    pop(): ChatMessage;
    /**
     * Gets all files contained in this history.
     *
     * @param client - LMStudio client
     */
    getAllFiles(client: LMStudioClient): Array<FileHandle>;
    /**
     * Allows iterating over the files in the history.
     */
    files(client: LMStudioClient): Generator<FileHandle>;
    /**
     * Returns true if this history contains any files.
     */
    hasFiles(): boolean;
    /**
     * Gets the message at the given index. If the index is negative, it will be counted from the end.
     *
     * If the index is out of bounds, this method will throw as oppose to returning undefined. This is
     * to help catch bugs early.
     */
    at(index: number): ChatMessage;
    /**
     * Allows iterating over the messages in the history.
     */
    [Symbol.iterator](): Generator<ChatMessage>;
    /**
     * Given a predicate, the predicate is called for each file in the history.
     *
     * - If the predicate returns true, the file is removed from the history and is collected into the
     *   returned array.
     * - If the predicate returns false, the file is kept in the history.
     *
     * This method is useful if you are implementing a preprocessor that needs to convert certain
     * types of files.
     *
     * If the predicate needs to be async, use the {@link Chat#consumeFilesAsync} method.
     *
     * @param client - LMStudio client
     * @param predicate - The predicate to call for each file.
     * @returns The files that were consumed.
     */
    consumeFiles(client: LMStudioClient, predicate: (file: FileHandle) => boolean): FileHandle[];
    /**
     * Given an async predicate, the predicate is called for each file in the history.
     *
     * - If the predicate returns true, the file is removed from the history and is collected into the
     *  returned array.
     * - If the predicate returns false, the file is kept in the history.
     *
     * This method is useful if you are implementing a preprocessor that needs to convert certain
     * types of files.
     *
     * If you need a synchronous version, use the {@link Chat#consumeFiles} method.
     *
     * @param client - LMStudio client
     * @param predicate - The predicate to call for each file.
     * @returns The files that were consumed.
     */
    consumeFilesAsync(client: LMStudioClient, predicate: (file: FileHandle) => Promise<boolean>): Promise<FileHandle[]>;
    getSystemPrompt(): string;
    replaceSystemPrompt(content: string): void;
    filterInPlace(predicate: (message: ChatMessage) => boolean): void;
    toString(): string;
}

/**
 * Options to use with {@link Chat#append}.
 *
 * @public
 */
export declare interface ChatAppendOpts {
    images?: Array<FileHandle>;
}

/**
 * @public
 */
export declare interface ChatHistoryData {
    messages: Array<ChatMessageData>;
}

/**
 * This type provides an easy way of specifying a chat history.
 *
 * Example:
 *
 * ```ts
 * const chat = Chat.from([
 *   { role: "user", content: "Hello" },
 *   { role: "assistant", content: "Hi" },
 *   { role: "user", content: "How are you?" },
 * ]);
 * ```
 *
 * @public
 */
export declare type ChatInput = Array<ChatMessageInput>;

/**
 * Represents anything that can be converted to a Chat. If you want to quickly construct a
 * Chat, use {@link ChatInput}.
 *
 * If a string is provided, it will be converted to a chat history with a single user message with
 * the provided text.
 *
 * @public
 */
export declare type ChatLike = ChatInput | string | Chat | ChatMessageInput | ChatHistoryData;

/**
 * Represents a single message in the history.
 *
 * @public
 */
export declare class ChatMessage extends MaybeMutable<ChatMessageData> {
    protected getClassName(): string;
    protected create(data: ChatMessageData, mutable: boolean): this;
    protected cloneData(data: ChatMessageData): ChatMessageData;
    protected constructor(data: ChatMessageData, mutable: boolean);
    /**
     * Create a mutable text only message.
     */
    static create(role: ChatMessageRoleData, content: string): ChatMessage;
    /**
     * Quickly create a mutable message with something that can be converted to a message.
     */
    static from(initializer: ChatMessageLike): ChatMessage;
    getRole(): "user" | "assistant" | "system" | "tool";
    setRole(role: ChatMessageRoleData): void;
    private getFileParts;
    /**
     * Gets all text contained in this message.
     */
    getText(): string;
    /**
     * Gets all files contained in this message.
     *
     * @param client - LMStudio client
     */
    getFiles(client: LMStudioClient): FileHandle[];
    /**
     * Allows iterating over the files in the message.
     */
    files(client: LMStudioClient): Generator<FileHandle>;
    /**
     * Given a predicate, the predicate is called for each file in the message.
     *
     * - If the predicate returns true, the file is removed from the message and is collected into the
     *   returned array.
     * - If the predicate returns false, the file is kept in the message.
     *
     * This method is useful if you are implementing a preprocessor that needs to convert certain
     * types of files.
     *
     * If the predicate needs to be async, use the {@link ChatMessage#consumeFilesAsync} method.
     *
     * @param client - LMStudio client
     * @param predicate - The predicate to call for each file.
     * @returns The files that were consumed.
     */
    consumeFiles(client: LMStudioClient, predicate: (file: FileHandle) => boolean): FileHandle[];
    /**
     * Given an async predicate, the predicate is called for each file in the message.
     *
     * - If the predicate returns true, the file is removed from the message and is collected into the
     *  returned array.
     * - If the predicate returns false, the file is kept in the message.
     *
     * This method is useful if you are implementing a preprocessor that needs to convert certain
     * types of files.
     *
     * If you need a synchronous version, use the {@link ChatMessage#consumeFiles} method.
     *
     * @param client - LMStudio client
     * @param predicate - The predicate to call for each file.
     * @returns The files that were consumed.
     */
    consumeFilesAsync(client: LMStudioClient, predicate: (file: FileHandle) => Promise<boolean>): Promise<FileHandle[]>;
    /**
     * Returns true if this message contains any files.
     */
    hasFiles(): boolean;
    /**
     * Append text to the message.
     */
    appendText(text: string): void;
    /**
     * Append a file to the message. Takes in a FileHandle. You can obtain a FileHandle from
     * `client.files.prepareImage`.
     */
    appendFile(file: FileHandle): void;
    /**
     * Replaces all text in the messages.
     *
     * If the message contains other components (such as files), they will kept. The replaced text
     * will be inserted to the beginning of the message.
     */
    replaceText(text: string): void;
    isSystemPrompt(): boolean;
    isUserMessage(): boolean;
    isAssistantMessage(): boolean;
    toString(): string;
}

/**
 * @public
 */
export declare type ChatMessageData = {
    role: "assistant";
    content: Array<ChatMessagePartTextData | ChatMessagePartFileData | ChatMessagePartToolCallRequestData>;
} | {
    role: "user";
    content: Array<ChatMessagePartTextData | ChatMessagePartFileData>;
} | {
    role: "system";
    content: Array<ChatMessagePartTextData | ChatMessagePartFileData>;
} | {
    role: "tool";
    content: Array<ChatMessagePartToolCallResultData>;
};

/**
 * This type provides an easy way of specifying a single chat message.
 *
 * @public
 */
export declare interface ChatMessageInput {
    /**
     * The sender of this message. Only "user", "assistant", and "system" is allowed. Defaults to
     * "user" if not specified.
     */
    role?: "user" | "assistant" | "system";
    /**
     * Text content of the message.
     */
    content?: string;
    /**
     * Images to be sent with the message to be used with vision models. To get a FileHandle, use
     * `client.files.prepareImage`.
     */
    images?: Array<FileHandle>;
}

/**
 * Represents something that can be converted to a ChatMessage.
 *
 * If a string is provided, it will be converted to a message sent by the user.
 *
 * @public
 */
export declare type ChatMessageLike = ChatMessageInput | string | ChatMessage | ChatMessageData;

/**
 * @public
 */
export declare type ChatMessagePartData = ChatMessagePartTextData | ChatMessagePartFileData | ChatMessagePartToolCallRequestData | ChatMessagePartToolCallResultData;

/**
 * @public
 */
export declare interface ChatMessagePartFileData {
    type: "file";
    /**
     * Original file name that is uploaded.
     */
    name: string;
    /**
     * Internal identifier for the file. Autogenerated, and is unique.
     */
    identifier: string;
    /**
     * Size of the file in bytes.
     */
    sizeBytes: number;
    /**
     * Type of the file.
     */
    fileType: FileType;
}

/**
 * @public
 */
export declare interface ChatMessagePartTextData {
    type: "text";
    text: string;
}

/**
 * @public
 */
export declare interface ChatMessagePartToolCallRequestData {
    type: "toolCallRequest";
    /**
     * Tool calls requested
     */
    toolCallRequest: ToolCallRequest;
}

/**
 * @public
 */
export declare interface ChatMessagePartToolCallResultData {
    type: "toolCallResult";
    /**
     * Result of a tool call
     */
    content: string;
    /**
     * The tool call ID that this result is for
     */
    toolCallId?: string;
}

/**
 * @public
 */
export declare type ChatMessageRoleData = "assistant" | "user" | "system" | "tool";

/**
 * Represents a source of a citation.
 *
 * @public
 */
export declare interface CitationSource {
    fileName: string;
    absoluteFilePath?: string;
    pageNumber?: number | [start: number, end: number];
    lineNumber?: number | [start: number, end: number];
}

/**
 * Theme color options.
 *
 * @public
 */
export declare type ColorPalette = "red" | "green" | "blue" | "yellow" | "orange" | "purple" | "default";

/**
 * @public
 */
export declare interface ConfigSchematics<TVirtualConfigSchematics extends VirtualConfigSchematics> {
    [configSchematicsBrand]?: TVirtualConfigSchematics;
}

declare const configSchematicsBrand: unique symbol;

/**
 * The opaque type for KVConfigSchematicsBuilder that is exposed in lmstudio.js SDK. Notably, this
 * has significantly simplified types and is easier to use.
 *
 * @public
 */
export declare interface ConfigSchematicsBuilder<TVirtualConfigSchematics extends VirtualConfigSchematics> {
    [configSchematicsBuilderBrand]?: TVirtualConfigSchematics;
    /**
     * Adds a field to the config schematics.
     */
    field<TKey extends string, TValueTypeKey extends keyof GlobalKVFieldValueTypeLibraryMap & string>(key: TKey, valueTypeKey: TValueTypeKey, valueTypeParams: GlobalKVFieldValueTypeLibraryMap[TValueTypeKey]["param"], defaultValue: GlobalKVFieldValueTypeLibraryMap[TValueTypeKey]["value"]): ConfigSchematicsBuilder<TVirtualConfigSchematics & {
        [key in TKey]: {
            key: TKey;
            type: GlobalKVFieldValueTypeLibraryMap[TValueTypeKey]["value"];
            valueTypeKey: TValueTypeKey;
        };
    }>;
    /**
     * Adds a "scope" to the config schematics. This is useful for grouping fields together.
     */
    scope<TScopeKey extends string, TInnerVirtualConfigSchematics extends VirtualConfigSchematics>(scopeKey: TScopeKey, fn: (builder: ConfigSchematicsBuilder<{}>) => ConfigSchematicsBuilder<TInnerVirtualConfigSchematics>): ConfigSchematicsBuilder<TVirtualConfigSchematics & {
        [InnerKey in keyof TInnerVirtualConfigSchematics & string as `${TScopeKey}.${InnerKey}`]: TInnerVirtualConfigSchematics[InnerKey];
    }>;
    build(): ConfigSchematics<TVirtualConfigSchematics>;
}

declare const configSchematicsBuilderBrand: unique symbol;

/**
 * Options to use with {@link PredictionProcessContentBlockController#appendText}.
 *
 * @public
 */
export declare interface ContentBlockAppendTextOpts {
    tokensCount?: number;
    fromDraftModel?: boolean;
}

/**
 * The style of a content block.
 *
 * @public
 */
export declare type ContentBlockStyle = {
    type: "default";
} | {
    type: "customLabel";
    label: string;
    color?: ColorPalette;
} | {
    type: "thinking";
    ended?: boolean;
    title?: string;
};

/**
 * Options to use with {@link ProcessingController#createCitationBlock}.
 *
 * @public
 */
export declare interface CreateCitationBlockOpts {
    fileName: string;
    fileIdentifier: string;
    pageNumber?: number | [start: number, end: number];
    lineNumber?: number | [start: number, end: number];
}

/**
 * @public
 */
export declare function createConfigSchematics(): ConfigSchematicsBuilder<{}>;

/**
 * Options to use with {@link ProcessingController#createContentBlock}.
 *
 * @public
 */
export declare interface CreateContentBlockOpts {
    includeInContext?: boolean;
    style?: ContentBlockStyle;
    prefix?: string;
    suffix?: string;
}

/**
 * @public
 */
export declare type DiagnosticsLogEvent = {
    timestamp: number;
    data: DiagnosticsLogEventData;
};

/**
 * @public
 */
export declare type DiagnosticsLogEventData = {
    type: "llm.prediction.input";
    modelPath: string;
    modelIdentifier: string;
    input: string;
};

/** @public */
export declare class DiagnosticsNamespace {
    private readonly diagnosticsPort;
    private readonly validator;
    /**
     * Register a callback to receive log events. Return a function to stop receiving log events.
     *
     * This method is in alpha. Do not use this method in production yet.
     * @alpha
     */
    unstable_streamLogs(listener: (logEvent: DiagnosticsLogEvent) => void): () => void;
}

/**
 * Options to use with {@link RepositoryNamespace#downloadArtifact}
 *
 * @public
 */
export declare interface DownloadArtifactOpts {
    owner: string;
    name: string;
    revisionNumber: number;
    /**
     * Where to save the artifact.
     */
    path: string;
    onProgress?: (update: DownloadProgressUpdate) => void;
    onStartFinalizing?: () => void;
    signal?: AbortSignal;
}

/** @public */
export declare interface DownloadOpts {
    onProgress?: (update: DownloadProgressUpdate) => void;
    onStartFinalizing?: () => void;
    signal?: AbortSignal;
}

/**
 * @public
 */
export declare interface DownloadProgressUpdate {
    downloadedBytes: number;
    totalBytes: number;
    speedBytesPerSecond: number;
}

/**
 * This represents a set of requirements for a model. It is not tied to a specific model, but rather
 * to a set of requirements that a model must satisfy.
 *
 * For example, if you got the model via `client.llm.get("my-identifier")`, you will get a
 * `LLMModel` for the model with the identifier `my-identifier`. If the model is unloaded, and
 * another model is loaded with the same identifier, using the same `LLMModel` will use the new
 * model.
 *
 * @public
 */
export declare abstract class DynamicHandle<TModelInstanceInfo extends ModelInstanceInfoBase> {
    /**
     * Gets the information of the model that is currently associated with this `DynamicHandle`. If no
     * model is currently associated, this will return `undefined`.
     *
     * Note: As models are loaded/unloaded, the model associated with this `LLMModel` may change at
     * any moment.
     */
    getModelInfo(): Promise<TModelInstanceInfo | undefined>;
    protected getLoadConfig(stack: string): Promise<KVConfig>;
}

/**
 * This represents a set of requirements for a model. It is not tied to a specific model, but rather
 * to a set of requirements that a model must satisfy.
 *
 * For example, if you got the model via `client.embedding.get("my-identifier")`, you will get a
 * `EmbeddingModel` for the model with the identifier `my-identifier`. If the model is unloaded, and
 * another model is loaded with the same identifier, using the same `EmbeddingModel` will use the
 * new model.
 *
 * @public
 */
export declare class EmbeddingDynamicHandle extends DynamicHandle<EmbeddingModelInstanceInfo> {
    embed(inputString: string): Promise<{
        embedding: Array<number>;
    }>;
    embed(inputStrings: Array<string>): Promise<Array<{
        embedding: Array<number>;
    }>>;
    getContextLength(): Promise<number>;
    getEvalBatchSize(): Promise<number>;
    tokenize(inputString: string): Promise<Array<number>>;
    tokenize(inputStrings: Array<string>): Promise<Array<Array<number>>>;
    countTokens(inputString: string): Promise<number>;
}

/**
 * @public
 */
export declare interface EmbeddingLoadModelConfig {
    gpu?: GPUSetting;
    contextLength?: number;
    ropeFrequencyBase?: number;
    ropeFrequencyScale?: number;
    keepModelInMemory?: boolean;
    tryMmap?: boolean;
}

/**
 * Represents a specific loaded Embedding. Most Embedding related operations are inherited from
 * {@link EmbeddingDynamicHandle}.
 *
 * @public
 */
export declare class EmbeddingModel extends EmbeddingDynamicHandle implements SpecificModel {
    readonly identifier: string;
    readonly path: string;
    readonly modelKey: string;
    readonly format: ModelCompatibilityType;
    readonly displayName: string;
    readonly sizeBytes: number;
    unload(): Promise<void>;
    getModelInfo(): Promise<EmbeddingModelInstanceInfo>;
}

/**
 * Embedding model specific information.
 *
 * @public
 */
export declare interface EmbeddingModelAdditionalInfo {
    /**
     * The maximum context length supported by the model.
     */
    maxContextLength: number;
}

/**
 * Info of an embedding model. It is a combination of {@link ModelInfoBase} and
 * {@link EmbeddingModelAdditionalInfo}.
 *
 * @public
 */
export declare type EmbeddingModelInfo = {
    type: "embedding";
} & ModelInfoBase & EmbeddingModelAdditionalInfo;

/**
 * Additional information of an embedding model instance.
 *
 * @public
 */
export declare interface EmbeddingModelInstanceAdditionalInfo {
    /**
     * The currently loaded context length.
     */
    contextLength: number;
}

/**
 * Info of a loaded embedding model instance. It is a combination of {@link ModelInstanceInfoBase},
 * {@link EmbeddingModelAdditionalInfo} and {@link EmbeddingModelInstanceAdditionalInfo}.
 *
 * @public
 */
export declare type EmbeddingModelInstanceInfo = {
    type: "embedding";
} & ModelInstanceInfoBase & EmbeddingModelAdditionalInfo & EmbeddingModelInstanceAdditionalInfo;

/** @public */
export declare class EmbeddingNamespace extends ModelNamespace<EmbeddingLoadModelConfig, EmbeddingModelInstanceInfo, EmbeddingModelInfo, EmbeddingDynamicHandle, EmbeddingModel> {
}

/**
 * Options to use with {@link RepositoryNamespace#ensureAuthenticated}.
 *
 * @public
 */
export declare interface EnsureAuthenticatedOpts {
    onAuthenticationUrl: (url: string) => void;
}

/**
 * Represents a file. Currently, the file can be either in the local file system or base64 encoded.
 *
 * @public
 */
export declare class FileHandle {
    readonly filesNamespace: FilesNamespace;
    readonly identifier: string;
    readonly type: FileType;
    readonly sizeBytes: number;
    /**
     * Original file name
     */
    readonly name: string;
    private readonly parsedIdentifier;
    /**
     * Gets the absolute file path of this file.
     */
    getFilePath(): Promise<string>;
    isImage(): boolean;
}

/**
 * @public
 *
 * The namespace for file-related operations.
 */
export declare class FilesNamespace {
    private readonly validator;
    /**
     * Adds a temporary image to LM Studio, and returns a FileHandle that can be used to reference
     * this image. This image will be deleted when the client disconnects.
     *
     * This method can only be used in environments that have file system access (such as Node.js).
     */
    prepareImage(path: string): Promise<FileHandle>;
    /**
     * Adds a temporary image to LM Studio. The content of the file is specified using base64. If you
     * are using Node.js and have a file laying around, consider using `prepareImage` instead.
     */
    prepareImageBase64(fileName: string, contentBase64: string): Promise<FileHandle>;
    /**
     * Adds a temporary file to LM Studio, and returns a FileHandle that can be used to reference this
     * file. This file will be deleted when the client disconnects.
     *
     * This method can only be used in environments that have file system access (such as Node.js).
     *
     * @deprecated Retrieval API is still in active development. Stay tuned for updates.
     */
    prepareFile(path: string): Promise<FileHandle>;
    /**
     * Adds a temporary file to LM Studio. The content of the file is specified using base64. If you
     * are using Node.js and have a file laying around, consider using `prepareFile` instead.
     *
     * @deprecated Retrieval API is still in active development. Stay tuned for updates.
     */
    prepareFileBase64(fileName: string, contentBase64: string): Promise<FileHandle>;
    /**
     * @deprecated Retrieval API is still in active development. Stay tuned for updates.
     */
    retrieve(query: string, files: Array<FileHandle>, opts?: RetrievalOpts): Promise<RetrievalResult>;
}

/**
 * @public
 *
 * TODO: Documentation
 */
export declare type FileType = "image" | "text/plain" | "application/pdf" | "application/word" | "text/other" | "unknown";

/**
 * A tool that is a function.
 *
 * @public
 */
export declare interface FunctionTool extends ToolBase {
    type: "function";
    parametersSchema: ZodSchema;
    implementation: (params: Record<string, unknown>) => any | Promise<any>;
}

/**
 * @public
 */
export declare interface FunctionToolCallRequest {
    id?: string;
    type: "function";
    arguments?: Record<string, any>;
    name: string;
}

/**
 * TODO: Documentation
 *
 * @public
 */
declare type Generator_2 = (ctl: GeneratorController) => Promise<void>;
export { Generator_2 as Generator }

/**
 * @public
 */
export declare type GeneratorController = Omit<ProcessingController, never>;

/**
 * @public
 */
export declare type GlobalKVFieldValueTypeLibraryMap = GlobalKVValueTypesLibrary extends KVFieldValueTypeLibrary<infer TKVFieldValueTypeLibraryMap> ? TKVFieldValueTypeLibraryMap : never;

/**
 * @public
 */
export declare type GlobalKVValueTypesLibrary = typeof kvValueTypesLibrary;

/**
 * Settings related to offloading work to the GPU.
 *
 * @public
 * @deprecated We are currently working on an improved way to control split. You can use this for
 * now. We will offer the alternative before this feature is removed.
 */
export declare type GPUSetting = {
    /**
     * A number between 0 to 1 representing the ratio of the work should be distributed to the GPU,
     * where 0 means no work is distributed and 1 means all work is distributed. Can also specify the
     * string "off" to mean 0 and the string "max" to mean 1.
     */
    ratio?: LLMLlamaAccelerationOffloadRatio;
    /**
     * The index of the GPU to use as the main GPU.
     */
    mainGpu?: number;
    /**
     * How to split computation across multiple GPUs.
     */
    splitStrategy?: LLMSplitStrategy;
    /**
     * Indices of GPUs to disable.
     */
    disabledGpus?: number[];
};

/**
 * Stringify options passed to actual implementations of stringify.
 *
 * @public
 */
export declare interface InnerFieldStringifyOpts {
    /**
     * Translate function.
     */
    t: (key: string, fallback: string) => string;
    /**
     * If exists, a soft cap on how long the stringified value should be.
     *
     * This does not have to be followed. Mostly used for fields like promptFormatTemplate where it
     * can grow very large.
     */
    desiredLength?: number;
}

/**
 * Represents a single field value type definition.
 *
 * @public
 */
export declare interface KVConcreteFieldValueType {
    paramType: ZodSchema;
    schemaMaker: (param: any) => ZodSchema;
    effectiveEquals: (a: any, b: any, typeParam: any) => boolean;
    stringify: (value: any, typeParam: any, opts: InnerFieldStringifyOpts) => string;
}

/**
 * @public
 */
export declare type KVConcreteFieldValueTypesMap = Map<string, KVConcreteFieldValueType>;

/**
 * TODO: Documentation
 *
 * @public
 */
export declare interface KVConfig {
    fields: Array<KVConfigField>;
}

/**
 * TODO: Documentation
 *
 * @public
 */
export declare interface KVConfigField {
    key: string;
    value?: any;
}

/**
 * @public
 */
export declare type KVConfigFieldDependency = {
    key: string;
    condition: {
        type: "equals";
        value: any;
    } | {
        type: "notEquals";
        value: any;
    };
};

/**
 * Represents a library of field value types.
 *
 * @public
 */
export declare class KVFieldValueTypeLibrary<TKVFieldValueTypeLibraryMap extends KVVirtualFieldValueTypesMapping> {
    private readonly valueTypes;
    constructor(valueTypes: KVConcreteFieldValueTypesMap);
    /**
     * Gets the schema for a specific field value type with the given key and parameters.
     */
    getSchema<TKey extends keyof TKVFieldValueTypeLibraryMap & string>(key: TKey, param: TKVFieldValueTypeLibraryMap[TKey]["param"]): ZodSchema<TKVFieldValueTypeLibraryMap[TKey]["value"]>;
    parseParamTypes<TKey extends keyof TKVFieldValueTypeLibraryMap & string>(key: TKey, param: any): TKVFieldValueTypeLibraryMap[TKey]["param"];
    effectiveEquals<TKey extends keyof TKVFieldValueTypeLibraryMap & string>(key: TKey, typeParam: TKVFieldValueTypeLibraryMap[TKey]["param"], a: TKVFieldValueTypeLibraryMap[TKey]["value"], b: TKVFieldValueTypeLibraryMap[TKey]["value"]): boolean;
    stringify<TKey extends keyof TKVFieldValueTypeLibraryMap & string>(key: TKey, typeParam: TKVFieldValueTypeLibraryMap[TKey]["param"], opts: InnerFieldStringifyOpts, value: TKVFieldValueTypeLibraryMap[TKey]["value"]): string;
}

/**
 * @public
 */
export declare const kvValueTypesLibrary: KVFieldValueTypeLibrary<{
    numeric: {
        value: number;
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
            min?: number | undefined;
            max?: number | undefined;
            step?: number | undefined;
            int?: boolean | undefined;
            precision?: number | undefined;
            slider?: {
                min: number;
                max: number;
                step: number;
            } | undefined;
            shortHand?: string | undefined;
        };
    };
} & {
    checkboxNumeric: {
        value: {
            value: number;
            checked: boolean;
        };
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
            min?: number | undefined;
            max?: number | undefined;
            step?: number | undefined;
            int?: boolean | undefined;
            precision?: number | undefined;
            slider?: {
                min: number;
                max: number;
                step: number;
            } | undefined;
            uncheckedHint?: string | undefined;
        };
    };
} & {
    string: {
        value: string;
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
            minLength?: number | undefined;
            maxLength?: number | undefined;
            isParagraph?: boolean | undefined;
            isProtected?: boolean | undefined;
            isToken?: boolean | undefined;
            placeholder?: string | undefined;
        };
    };
} & {
    select: {
        value: string;
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: {
                key: string;
                condition: {
                    type: "equals";
                    value: any;
                } | {
                    type: "notEquals";
                    value: any;
                };
            }[] | undefined;
            options: (string | {
                value: string;
                displayName: string;
            })[];
        };
    };
} & {
    boolean: {
        value: boolean;
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    stringArray: {
        value: string[];
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
            maxNumItems?: number | undefined;
            allowEmptyStrings?: boolean | undefined;
        };
    };
} & {
    numericArray: {
        value: number[];
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
            min?: number | undefined;
            max?: number | undefined;
            int?: boolean | undefined;
        };
    };
} & {
    contextOverflowPolicy: {
        value: "stopAtLimit" | "truncateMiddle" | "rollingWindow";
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    context: {
        value: ({
            type: "jsonFile";
            absPath: string;
        } | {
            type: "yamlFile";
            absPath: string;
        })[];
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    contextLength: {
        value: number;
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
            max?: number | undefined;
        };
    };
} & {
    modelIdentifier: {
        value: string;
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
            domain?: ("llm" | "embedding" | "imageGen" | "transcription" | "tts")[] | undefined;
        };
    };
} & {
    llmPromptTemplate: {
        value: {
            type: "manual" | "jinja";
            stopStrings: string[];
            manualPromptTemplate?: {
                beforeSystem: string;
                afterSystem: string;
                beforeUser: string;
                afterUser: string;
                beforeAssistant: string;
                afterAssistant: string;
            } | undefined;
            jinjaPromptTemplate?: {
                template: string;
                bosToken: string;
                eosToken: string;
                inputConfig: {
                    messagesConfig: {
                        contentConfig: {
                            type: "string";
                            imagesConfig?: {
                                value: string;
                                type: "simple";
                            } | {
                                type: "numbered";
                                prefix: string;
                                suffix: string;
                            } | {
                                type: "object";
                            } | undefined;
                        } | {
                            type: "array";
                            textFieldName: "text" | "content";
                            imagesConfig?: {
                                value: string;
                                type: "simple";
                            } | {
                                type: "numbered";
                                prefix: string;
                                suffix: string;
                            } | {
                                type: "object";
                            } | undefined;
                        };
                    };
                    useTools: boolean;
                };
            } | undefined;
        };
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    llmReasoningParsing: {
        value: {
            enabled: boolean;
            startString: string;
            endString: string;
        };
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    llamaStructuredOutput: {
        value: {
            type: "none" | "json" | "gbnf";
            jsonSchema?: any;
            gbnfGrammar?: string | undefined;
        };
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    speculativeDecodingDraftModel: {
        value: string;
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    toolUse: {
        value: {
            type: "none";
        } | {
            type: "toolArray";
            tools?: {
                function: {
                    name: string;
                    description?: string | undefined;
                    parameters?: {
                        type: "object";
                        properties: Record<string, any>;
                        required?: string[] | undefined;
                        additionalProperties?: boolean | undefined;
                    } | undefined;
                };
                type: "function";
            }[] | undefined;
        };
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    llamaAccelerationOffloadRatio: {
        value: number | "max" | "off";
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
            numLayers?: number | undefined;
        };
    };
} & {
    llamaMirostatSampling: {
        value: {
            version: 0 | 1 | 2;
            learningRate: number;
            targetEntropy: number;
        };
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    llamaLogitBias: {
        value: [number, number | "-inf"][];
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    llamaCacheQuantizationType: {
        value: {
            value: "f32" | "f16" | "q8_0" | "q4_0" | "q4_1" | "iq4_nl" | "q5_0" | "q5_1";
            checked: boolean;
        };
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    mlxKvCacheQuantizationType: {
        value: {
            enabled: boolean;
            bits: 2 | 3 | 4 | 6 | 8;
            groupSize: 32 | 64 | 128;
            quantizedStart: number;
        };
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    retrievalChunkingMethod: {
        value: {
            type: "recursive-v1";
            chunkSize: number;
            chunkOverlap: number;
        };
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    envVars: {
        value: Partial<Record<"HSA_OVERRIDE_GFX_VERSION", string>>;
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
} & {
    gpuSplitConfig: {
        value: {
            disabledGpus: number[];
            strategy: "custom" | "evenly" | "priorityOrder";
            priority: number[];
            customRatio: number[];
        };
        param: {
            displayName?: string | undefined;
            hint?: string | undefined;
            modelCentric?: boolean | undefined;
            nonConfigurable?: boolean | undefined;
            machineDependent?: boolean | undefined;
            warning?: string | undefined;
            isExperimental?: boolean | undefined;
            dependencies?: KVConfigFieldDependency[] | undefined;
        };
    };
}>;

/**
 * Used internally by KVFieldValueTypesLibrary to keep track of a single field value type definition
 * with the generics.
 *
 * @public
 */
export declare interface KVVirtualFieldValueType {
    value: any;
    param: any;
}

/**
 * Used internally by KVFieldValueTypesLibrary to keep track of all field value type definitions
 * with the generics.
 *
 * @public
 */
export declare type KVVirtualFieldValueTypesMapping = {
    [key: string]: KVVirtualFieldValueType;
};

/**
 * Represents a specific loaded LLM. Most LLM related operations are inherited from
 * {@link LLMDynamicHandle}.
 *
 * @public
 */
export declare class LLM extends LLMDynamicHandle implements SpecificModel {
    readonly identifier: string;
    readonly path: string;
    readonly modelKey: string;
    readonly format: ModelCompatibilityType;
    readonly displayName: string;
    readonly sizeBytes: number;
    readonly vision: boolean;
    readonly trainedForToolUse: boolean;
    unload(): Promise<void>;
    getModelInfo(): Promise<LLMInstanceInfo>;
}

/**
 * Options for {@link LLMDynamicHandle#act}.
 *
 * @public
 */
export declare interface LLMActionOpts<TStructuredOutputType = unknown> extends LLMPredictionConfigInput<TStructuredOutputType> {
    /**
     * A callback that is called when the model has output the first token of a prediction. This
     * callback is called with round index (the index of the prediction within `.act(...)`,
     * 0-indexed).
     */
    onFirstToken?: (roundIndex: number) => void;
    /**
     * A callback for each fragment that is output by the model. This callback is called with the
     * fragment that is emitted. The fragment itself is augmented with the round index (the index of
     * the prediction within `.act(...)`, 0-indexed).
     *
     * For example, for an `.act` invocation with 2 predictions, the callback may be called in the
     * following sequence.
     *
     * - `{ roundIndex: 0, content: "f1", ... }` when the first prediction emits `f1`.
     * - `{ roundIndex: 0, content: "f2", ... }` when the first prediction emits `f2`.
     * - `{ roundIndex: 1, content: "f3", ... }` when the second prediction emits `f3`.
     * - `{ roundIndex: 1, content: "f4", ... }` when the second prediction emits `f4`.
     */
    onPredictionFragment?: (fragment: LLMPredictionFragmentWithRoundIndex) => void;
    /**
     * A callback that is called when a message is generated and should be added to the Chat. This is
     * useful if you want to add the generated content to a chat so you can continue the conversation.
     *
     * Note that, during one `act` call, multiple messages may be generated, and this callback
     * will be called multiple times. For example, if the model requests to use a tool during the
     * first prediction and stops after the second prediction, three messages will be created (and
     * thus this callback will be called three times):
     *
     * 1. The first prediction's generated message, which contains information about the tool request.
     * 2. The result of running the tool.
     * 3. The second prediction's generated message.
     */
    onMessage?: (message: ChatMessage) => void;
    /**
     * A callback that will be called when a new round of prediction starts.
     */
    onRoundStart?: (roundIndex: number) => void;
    /**
     * A callback that will be called when a round of prediction ends.
     */
    onRoundEnd?: (roundIndex: number) => void;
    /**
     * A callback that will be called when a prediction in a round is completed. The callback is
     * called with the result of the prediction. You can access the roundIndex via the `.roundIndex`
     * property. (See {@link PredictionResult} for more info).
     *
     * Note: this is called immediately after the prediction is completed. The tools may still be
     * running.
     */
    onPredictionCompleted?: (predictionResult: PredictionResult) => void;
    /**
     * A callback that is called when the model is processing the prompt. The callback is called with
     * the round index (the index of the prediction within `.act(...)`, 0-indexed) and a number
     * between 0 and 1, representing the progress of the prompt processing.
     *
     * For example, for an `.act` invocation with 2 prediction rounds, the callback may be called
     * in the following sequence.
     *
     * - `(0, 0.3)` when the first prediction's prompt processing is 30% done.
     * - `(0, 0.7)` when the first prediction's prompt processing is 70% done.
     * - ... The model starts to stream the first prediction's output, during which, this callback is
     *   not called.
     * - `(1, 0.3)` when the second prediction's prompt processing is 50% done.
     * - `(1, 0.7)` when the second prediction's prompt processing is 70% done.
     */
    onPromptProcessingProgress?: (roundIndex: number, progress: number) => void;
    /**
     * A handler that is called when a tool request is made by the model but is invalid.
     *
     * There are multiple ways for a tool request to be invalid. For example, the model can simply
     * output a string that claims to be a tool request, but cannot at all be parsed as one. Or it may
     * request to use a tool that doesn't exist, or the parameters provided are invalid.
     *
     * When this happens, LM Studio will provide why it failed in the error parameter. We will also
     * try to parse the tool request and provide it as the second parameter. However, this is not
     * guaranteed to success, and the second parameter may be `undefined`.
     *
     * If we successfully parsed the request (thus the request parameter is not undefined), anything
     * returned in this callback will be used as the result of the tool call. This is useful for
     * providing a error message to the model so it may try again. However, if nothing (undefined) is
     * returned, LM Studio will not provide a result to the given tool call.
     *
     * If we failed to parsed the request (thus the request parameter is undefined), the return value
     * of this callback will be ignored as LM Studio cannot provide results to a tool call that has
     * failed to parse.
     *
     * If you decide the failure is too severe to continue, you can always throw an error in this
     * callback, which will immediately fail the `.act` call with the same error you provided.
     *
     * By default, we use the following implementation:
     *
     * ```ts
     * handleInvalidToolRequest: (error, request) => {
     *   if (request) {
     *     return error.message;
     *   }
     *   throw error;
     * },
     * ```
     *
     * The default handler will do the following: If the model requested a tool that can be parsed but
     * is still invalid, we will return the error message as the result of the tool call. If the model
     * requested a tool that cannot be parsed, we will throw an error, which will immediately fail the
     * `.act` call.
     *
     * Note, when an invalid tool request occurs due to parameters type mismatch, we will never call
     * the original tool automatically due to security considerations. If you do decide to call the
     * original tool, you can do so manually within this callback.
     *
     * This callback can also be async.
     */
    handleInvalidToolRequest?: (error: Error, request: ToolCallRequest | undefined) => any | Promise<any>;
    /**
     * Limit the number of prediction rounds that the model can perform. In the last prediction, the
     * model will not be allowed to use more tools.
     *
     * Note, some models may requests multiple tool calls within a single prediction round. This
     * option only limits the number of prediction rounds, not the total number of tool calls.
     */
    maxPredictionRounds?: number;
    /**
     * An abort signal that can be used to cancel the prediction.
     */
    signal?: AbortSignal;
    /**
     * Which preset to use.
     *
     * @remarks
     *
     * This preset selection is "layered" between your overrides and the "server session" config.
     * Which means, other fields you specify in this opts object will override the preset, while the
     * preset content will override the "server session" config.
     */
    preset?: string;
}

/**
 * LLM specific information.
 *
 * @public
 */
export declare interface LLMAdditionalInfo {
    /**
     * Whether this model is vision-enabled (i.e. supports image input).
     */
    vision: boolean;
    /**
     * Whether this model is trained natively for tool use.
     */
    trainedForToolUse: boolean;
    /**
     * Maximum context length of the model.
     */
    maxContextLength: number;
}

/**
 * Options for applying a prompt template.
 * @public
 */
export declare interface LLMApplyPromptTemplateOpts {
    /**
     * Whether to omit the BOS token when formatting.
     *
     * Default: false
     */
    omitBosToken?: boolean;
    /**
     * Whether to omit the EOS token when formatting.
     *
     * Default: false
     */
    omitEosToken?: boolean;
}

/**
 * Behavior for when the generated tokens length exceeds the context window size. Only the following
 * values are allowed:
 *
 * - `stopAtLimit`: Stop the prediction when the generated tokens length exceeds the context window
 *   size. If the generation is stopped because of this limit, the `stopReason` in the prediction
 *   stats will be set to `contextLengthReached`.
 * - `truncateMiddle`: Keep the system prompt and the first user message, truncate middle.
 * - `rollingWindow`: Maintain a rolling window and truncate past messages.
 *
 * @public
 */
export declare type LLMContextOverflowPolicy = "stopAtLimit" | "truncateMiddle" | "rollingWindow";

/**
 * This represents a set of requirements for a model. It is not tied to a specific model, but rather
 * to a set of requirements that a model must satisfy.
 *
 * For example, if you got the model via `client.llm.model("my-identifier")`, you will get a
 * `LLMDynamicHandle` for the model with the identifier `my-identifier`. If the model is unloaded,
 * and another model is loaded with the same identifier, using the same `LLMDynamicHandle` will use
 * the new model.
 *
 * @public
 */
export declare class LLMDynamicHandle extends DynamicHandle<LLMInstanceInfo> {
    private predictionConfigInputToKVConfig;
    private createZodParser;
    /**
     * Use the loaded model to predict text.
     *
     * This method returns an {@link OngoingPrediction} object. An ongoing prediction can be used as a
     * promise (if you only care about the final result) or as an async iterable (if you want to
     * stream the results as they are being generated).
     *
     * Example usage as a promise (Resolves to a {@link PredictionResult}):
     *
     * ```typescript
     * const result = await model.complete("When will The Winds of Winter be released?");
     * console.log(result.content);
     * ```
     *
     * Or
     *
     * ```typescript
     * model.complete("When will The Winds of Winter be released?")
     *  .then(result =\> console.log(result.content))
     *  .catch(error =\> console.error(error));
     * ```
     *
     * Example usage as an async iterable (streaming):
     *
     * ```typescript
     * for await (const { content } of model.complete("When will The Winds of Winter be released?")) {
     *   process.stdout.write(content);
     * }
     * ```
     *
     * If you wish to stream the result, but also getting the final prediction results (for example,
     * you wish to get the prediction stats), you can use the following pattern:
     *
     * ```typescript
     * const prediction = model.complete("When will The Winds of Winter be released?");
     * for await (const { content } of prediction) {
     *   process.stdout.write(content);
     * }
     * const result = await prediction.result();
     * console.log(result.stats);
     * ```
     *
     * @param prompt - The prompt to use for prediction.
     * @param opts - Options for the prediction.
     */
    complete<TStructuredOutputType>(prompt: string, opts?: LLMPredictionOpts<TStructuredOutputType>): OngoingPrediction<TStructuredOutputType>;
    private resolveCompletionContext;
    /**
     * Use the loaded model to generate a response based on the given history.
     *
     * This method returns an {@link OngoingPrediction} object. An ongoing prediction can be used as a
     * promise (if you only care about the final result) or as an async iterable (if you want to
     * stream the results as they are being generated).
     *
     * Example usage as a promise (Resolves to a {@link PredictionResult}):
     *
     * ```typescript
     * const history = [{ role: 'user', content: "When will The Winds of Winter be released?" }];
     * const result = await model.respond(history);
     * console.log(result.content);
     * ```
     *
     * Or
     *
     * ```typescript
     * const history = [{ role: 'user', content: "When will The Winds of Winter be released?" }];
     * model.respond(history)
     *  .then(result => console.log(result.content))
     *  .catch(error => console.error(error));
     * ```
     *
     * Example usage as an async iterable (streaming):
     *
     * ```typescript
     * const history = [{ role: 'user', content: "When will The Winds of Winter be released?" }];
     * for await (const { content } of model.respond(history)) {
     *   process.stdout.write(content);
     * }
     * ```
     *
     * If you wish to stream the result, but also getting the final prediction results (for example,
     * you wish to get the prediction stats), you can use the following pattern:
     *
     * ```typescript
     * const history = [{ role: 'user', content: "When will The Winds of Winter be released?" }];
     * const prediction = model.respond(history);
     * for await (const { content } of prediction) {
     *   process.stdout.write(content);
     * }
     * const result = await prediction;
     * console.log(result.stats);
     * ```
     *
     * @param chat - The LLMChatHistory array to use for generating a response.
     * @param opts - Options for the prediction.
     */
    respond<TStructuredOutputType>(chat: ChatLike, opts?: LLMRespondOpts<TStructuredOutputType>): OngoingPrediction<TStructuredOutputType>;
    /**
     * @param chat - The LLMChatHistory array to act from as the base
     * @param tool - An array of tools that the model can use during the operation. You can create
     * tools by using the `tool` function.
     * @param opts - Additional options
     *
     * Example:
     *
     * ```
     * import { LMStudioClient, tool } from "@lmstudio/sdk";
     * import { z } from "zod";
     *
     * const client = new LMStudioClient();
     * const model = await client.llm.model();
     *
     * const additionTool = tool({
     *   name: "add",
     *   description: "Add two numbers",
     *   parameters: {
     *     a: z.number(),
     *     b: z.number(),
     *   },
     *   implementation: ({ a, b }) => a + b,
     * });
     *
     * await model.act("What is 1234 + 4321?", [additionTool], {
     *   onMessage: message => console.log(message.toString()),
     * });
     * ```
     */
    act(chat: ChatLike, tools: Array<Tool>, opts?: LLMActionOpts): Promise<ActResult>;
    getContextLength(): Promise<number>;
    applyPromptTemplate(history: ChatLike, opts?: LLMApplyPromptTemplateOpts): Promise<string>;
    tokenize(inputString: string): Promise<Array<number>>;
    tokenize(inputStrings: Array<string>): Promise<Array<Array<number>>>;
    countTokens(inputString: string): Promise<number>;
    /**
     * Starts to eagerly preload a draft model. This is useful when you want a draft model to be ready
     * for speculative decoding.
     *
     * Preloading is done on a best-effort basis and may not always succeed. It is not guaranteed that
     * the draft model is actually loaded when this method returns. Thus, this method should only be
     * used as an optimization. The actual draft model used only depends on the parameter set when
     * performing the prediction.
     */
    unstable_preloadDraftModel(draftModelKey: string): Promise<void>;
}

/**
 * @public
 */
export declare interface LLMGenInfo {
    indexedModelIdentifier: string;
    identifier: string;
    loadModelConfig: KVConfig;
    predictionConfig: KVConfig;
    stats: LLMPredictionStats;
}

/**
 * Info of an LLM. It is a combination of {@link ModelInfoBase} and {@link LLMAdditionalInfo}.
 *
 * @public
 */
export declare type LLMInfo = {
    type: "llm";
} & ModelInfoBase & LLMAdditionalInfo;

/**
 * Additional information of an LLM instance.
 *
 * @public
 */
export declare interface LLMInstanceAdditionalInfo {
    contextLength: number;
}

/**
 * Info of a loaded LLM instance. It is a combination of {@link ModelInstanceInfoBase},
 * {@link LLMAdditionalInfo} and {@link LLMInstanceAdditionalInfo}.
 *
 * @public
 */
export declare type LLMInstanceInfo = {
    type: "llm";
} & ModelInstanceInfoBase & LLMAdditionalInfo & LLMInstanceAdditionalInfo;

/**
 *
 * Configures how ChatHistoryData should be input to jinja for prompt rendering.
 *
 * @public
 */
export declare interface LLMJinjaInputConfig {
    messagesConfig: LLMJinjaInputMessagesConfig;
    useTools: boolean;
}

/**
 * Configures how ChatHistoryMessages should be input to jinja for prompt rendering.
 * @public
 */
export declare interface LLMJinjaInputMessagesConfig {
    contentConfig: LLMJinjaInputMessagesContentConfig;
}

/**
 * Configures how content in ChatHistoryMessages should be input to jinja for prompt rendering.
 *
 * ### string
 *
 * Content is represented as a single string,
 *
 * i.e.: `{ role: "user", content: "Hello" }`
 *
 * ### array
 * Content is represented as an array of typed parts,
 *
 * i.e.: `{ role: "user", content: [{ type: "text", text: "Hello" }] }`
 *
 * @public
 */
export declare type LLMJinjaInputMessagesContentConfig = {
    type: "string";
    imagesConfig?: LLMJinjaInputMessagesContentImagesConfig;
} | {
    type: "array";
    textFieldName: LLMJinjaInputMessagesContentConfigTextFieldName;
    imagesConfig?: LLMJinjaInputMessagesContentImagesConfig;
};

/**
 * Possible field names for text in jinja input messages (when content is an array).
 * @public
 */
export declare type LLMJinjaInputMessagesContentConfigTextFieldName = "content" | "text";

/**
 * Configures how images in ChatHistoryMessages should be input to jinja for prompt rendering.
 *
 * ### simple
 * Images are represented in text as a single static string. I.e: "\<image\>"
 *
 * ### numbered
 * Images are represented in text as numbered strings with numbers between a prefix and suffix.
 * I.e.: "\<image_1\>","\<image_2\>", etc.
 *
 * ### object
 * Images are represented as an object in the jinja render input. I.e.: \{ type: "image" \}
 * @public
 */
export declare type LLMJinjaInputMessagesContentImagesConfig = {
    type: "simple";
    value: string;
} | {
    type: "numbered";
    prefix: string;
    suffix: string;
} | {
    type: "object";
};

/**
 * @public
 */
export declare interface LLMJinjaPromptTemplate {
    template: string;
    /**
     * Required for applying Jinja template.
     */
    bosToken: string;
    /**
     * Required for applying Jinja template.
     */
    eosToken: string;
    /**
     * Config for how ChatHistoryData should be input to jinja for prompt rendering.
     */
    inputConfig: LLMJinjaInputConfig;
}

/**
 * How much of the model's work should be offloaded to the GPU. The value should be between 0 and 1.
 * A value of 0 means that no layers are offloaded to the GPU, while a value of 1 means that all
 * layers (that can be offloaded) are offloaded to the GPU.
 *
 * @public
 */
export declare type LLMLlamaAccelerationOffloadRatio = number | "max" | "off";

/**
 * TODO: Add documentation
 *
 * @public
 */
export declare type LLMLlamaCacheQuantizationType = "f32" | "f16" | "q8_0" | "q4_0" | "q4_1" | "iq4_nl" | "q5_0" | "q5_1";

/** @public */
export declare interface LLMLoadModelConfig {
    /**
     * How to distribute the work to your GPUs. See {@link GPUSetting} for more information.
     *
     * @public
     * @deprecated We are currently working on an improved way to control split. You can use this for
     * now but expect breakage in the future.
     */
    gpu?: GPUSetting;
    /**
     * If set to true, detected system limits for VRAM will be strictly enforced. If a model + gpu
     * offload combination would exceed the detected available VRAM, model offload will be capped to
     * not exceed the available VRAM.
     *
     * @public
     */
    gpuStrictVramCap?: boolean;
    /**
     * The size of the context length in number of tokens. This will include both the prompts and the
     * responses. Once the context length is exceeded, the value set in
     * {@link LLMPredictionConfigBase#contextOverflowPolicy} is used to determine the behavior.
     *
     * See {@link LLMContextOverflowPolicy} for more information.
     */
    contextLength?: number;
    /**
     * Custom base frequency for rotary positional embeddings (RoPE).
     *
     * This advanced parameter adjusts how positional information is embedded in the model's
     * representations. Increasing this value may enable better performance at high context lengths by
     * modifying how the model processes position-dependent information.
     */
    ropeFrequencyBase?: number;
    /**
     * Scaling factor for RoPE (Rotary Positional Encoding) frequency.
     *
     * This factor scales the effective context window by modifying how positional information is
     * encoded. Higher values allow the model to handle longer contexts by making positional encoding
     * more granular, which can be particularly useful for extending a model beyond its original
     * training context length.
     */
    ropeFrequencyScale?: number;
    /**
     * Number of input tokens to process together in a single batch during evaluation.
     *
     * Increasing this value typically improves processing speed and throughput by leveraging
     * parallelization, but requires more memory. Finding the optimal batch size often involves
     * balancing between performance gains and available hardware resources.
     */
    evalBatchSize?: number;
    /**
     * Enables Flash Attention for optimized attention computation.
     *
     * Flash Attention is an efficient implementation that reduces memory usage and speeds up
     * generation by optimizing how attention mechanisms are computed. This can significantly
     * improve performance on compatible hardware, especially for longer sequences.
     */
    flashAttention?: boolean;
    /**
     * When enabled, prevents the model from being swapped out of system memory.
     *
     * This option reserves system memory for the model even when portions are offloaded to GPU,
     * ensuring faster access times when the model needs to be used. Improves performance
     * particularly for interactive applications, but increases overall RAM requirements.
     */
    keepModelInMemory?: boolean;
    /**
     * Random seed value for model initialization to ensure reproducible outputs.
     *
     * Setting a specific seed ensures that random operations within the model (like sampling)
     * produce the same results across different runs, which is important for reproducibility
     * in testing and development scenarios.
     */
    seed?: number;
    /**
     * When enabled, stores the key-value cache in half-precision (FP16) format.
     *
     * This option significantly reduces memory usage during inference by using 16-bit floating
     * point numbers instead of 32-bit for the attention cache. While this may slightly reduce
     * numerical precision, the impact on output quality is generally minimal for most applications.
     */
    useFp16ForKVCache?: boolean;
    /**
     * Attempts to use memory-mapped (mmap) file access when loading the model.
     *
     * Memory mapping can improve initial load times by mapping model files directly from disk to
     * memory, allowing the operating system to handle paging. This is particularly beneficial for
     * quick startup, but may reduce performance if the model is larger than available system RAM,
     * causing frequent disk access.
     */
    tryMmap?: boolean;
    /**
     * Specifies the number of experts to use for models with Mixture of Experts (MoE) architecture.
     *
     * MoE models contain multiple "expert" networks that specialize in different aspects of the task.
     * This parameter controls how many of these experts are active during inference, affecting both
     * performance and quality of outputs. Only applicable for models designed with the MoE
     * architecture.
     */
    numExperts?: number;
    /**
     * Quantization type for the Llama model's key cache.
     *
     * This option determines the precision level used to store the key component of the attention
     * mechanism's cache. Lower precision values (e.g., 4-bit or 8-bit quantization) significantly
     * reduce memory usage during inference but may slightly impact output quality. The effect varies
     * between different models, with some being more robust to quantization than others.
     *
     * Set to false to disable quantization and use full precision.
     */
    llamaKCacheQuantizationType?: LLMLlamaCacheQuantizationType | false;
    /**
     * Quantization type for the Llama model's value cache.
     *
     * Similar to the key cache quantization, this option controls the precision used for the value
     * component of the attention mechanism's cache. Reducing precision saves memory but may affect
     * generation quality. This option requires Flash Attention to be enabled to function properly.
     *
     * Different models respond differently to value cache quantization, so experimentation may be
     * needed to find the optimal setting for a specific use case. Set to false to disable
     * quantization.
     */
    llamaVCacheQuantizationType?: LLMLlamaCacheQuantizationType | false;
}

/**
 * @public
 */
export declare interface LLMManualPromptTemplate {
    /**
     * String to be prepended to the system prompt.
     */
    beforeSystem: string;
    /**
     * String to be appended to the system prompt.
     */
    afterSystem: string;
    /**
     * String to be prepended to a user message.
     */
    beforeUser: string;
    /**
     * String to be appended to a user message.
     */
    afterUser: string;
    /**
     * String to be prepended to an assistant message.
     */
    beforeAssistant: string;
    /**
     * String to be appended to an assistant message.
     */
    afterAssistant: string;
}

/** @public */
export declare class LLMNamespace extends ModelNamespace<LLMLoadModelConfig, LLMInstanceInfo, LLMInfo, LLMDynamicHandle, LLM> {
}

/**
 * @public
 */
export declare type LLMPredictionConfig = Omit<LLMPredictionConfigInput<any>, "structured"> & {
    structured?: LLMStructuredPredictionSetting;
};

/**
 * Shared config for running predictions on an LLM.
 *
 * @public
 */
export declare interface LLMPredictionConfigInput<TStructuredOutputType = unknown> {
    /**
     * Number of tokens to predict at most. If set to false, the model will predict as many tokens as
     * it wants.
     *
     * When the prediction is stopped because of this limit, the `stopReason` in the prediction stats
     * will be set to `maxPredictedTokensReached`.
     *
     * See {@link LLMPredictionStopReason} for other reasons that a prediction might stop.
     */
    maxTokens?: number | false;
    /**
     * The temperature parameter for the prediction model. A higher value makes the predictions more
     * random, while a lower value makes the predictions more deterministic. The value should be
     * between 0 and 1.
     */
    temperature?: number;
    /**
     * An array of strings. If the model generates one of these strings, the prediction will stop.
     *
     * When the prediction is stopped because of this limit, the `stopReason` in the prediction stats
     * will be set to `stopStringFound`.
     *
     * See {@link LLMPredictionStopReason} for other reasons that a prediction might stop.
     */
    stopStrings?: Array<string>;
    /**
     * An array of strings. If the model generates one of these strings, the prediction will stop with
     * the `stopReason` `toolCalls`.
     *
     * See {@link LLMPredictionStopReason} for other reasons that a prediction might stop.
     */
    toolCallStopStrings?: Array<string>;
    /**
     * The behavior for when the generated tokens length exceeds the context window size. The allowed
     * values are:
     *
     * - `stopAtLimit`: Stop the prediction when the generated tokens length exceeds the context
     *   window size. If the generation is stopped because of this limit, the `stopReason` in the
     *   prediction stats will be set to `contextLengthReached`
     * - `truncateMiddle`: Keep the system prompt and the first user message, truncate middle.
     * - `rollingWindow`: Maintain a rolling window and truncate past messages.
     */
    contextOverflowPolicy?: LLMContextOverflowPolicy;
    /**
     * Configures the model to output structured JSON data that follows a specific schema defined
     * using Zod.
     *
     * When you provide a Zod schema, the model will be instructed to generate JSON that conforms to
     * that schema rather than free-form text.
     *
     * This is particularly useful for extracting specific data points from model responses or when
     * you need the output in a format that can be directly used by your application.
     */
    structured?: {
        /**
         * IMPORTANT
         *
         * When passing in a zod schema as the structured generation option, you must provide an
         * actual zod schema object. (returned by z.something()). The type here only requires an
         * object with a `parse` function. This is not enough! We need an actual zod schema because
         * we will need to extract the JSON schema from it. If you don't want use zod, consider
         * passing in a `LLMStructuredPredictionSetting` instead.
         *
         * The reason we only have a `parse` function here (as oppose to actually requiring
         * ZodType<TStructuredOutputType> is due to this zod bug causing TypeScript breakage, when
         * multiple versions of zod exist.
         *
         * - https://github.com/colinhacks/zod/issues/577
         * - https://github.com/colinhacks/zod/issues/2697
         * - https://github.com/colinhacks/zod/issues/3435
         */
        parse: (input: any) => TStructuredOutputType;
    } | LLMStructuredPredictionSetting;
    /**
     * @deprecated Raw tools are currently not well-supported. It may or may not work. If you want to
     * use tools, use `model.act` instead.
     */
    rawTools?: LLMToolUseSetting;
    /**
     * Controls token sampling diversity by limiting consideration to the K most likely next tokens.
     *
     * For example, if set to 40, only the 40 tokens with the highest probabilities will be considered
     * for the next token selection. A lower value (e.g., 20) will make the output more focused and
     * conservative, while a higher value (e.g., 100) allows for more creative and diverse outputs.
     *
     * Typical values range from 20 to 100.
     */
    topKSampling?: number;
    /**
     * Applies a penalty to repeated tokens to prevent the model from getting stuck in repetitive
     * patterns.
     *
     * A value of 1.0 means no penalty. Values greater than 1.0 increase the penalty. For example, 1.2
     * would reduce the probability of previously used tokens by 20%. This is particularly useful for
     * preventing the model from repeating phrases or getting stuck in loops.
     *
     * Set to false to disable the penalty completely.
     */
    repeatPenalty?: number | false;
    /**
     * Sets a minimum probability threshold that a token must meet to be considered for generation.
     *
     * For example, if set to 0.05, any token with less than 5% probability will be excluded from
     * consideration. This helps filter out unlikely or irrelevant tokens, potentially improving
     * output quality.
     *
     * Value should be between 0 and 1. Set to false to disable this filter.
     */
    minPSampling?: number | false;
    /**
     * Implements nucleus sampling by only considering tokens whose cumulative probabilities reach a
     * specified threshold.
     *
     * For example, if set to 0.9, the model will consider only the most likely tokens that together
     * add up to 90% of the probability mass. This helps balance between diversity and quality by
     * dynamically adjusting the number of tokens considered based on their probability distribution.
     *
     * Value should be between 0 and 1. Set to false to disable nucleus sampling.
     */
    topPSampling?: number | false;
    /**
     * Controls how often the XTC (Exclude Top Choices) sampling technique is applied during
     * generation.
     *
     * XTC sampling can boost creativity and reduce clichés by occasionally filtering out common
     * tokens. For example, if set to 0.3, there's a 30% chance that XTC sampling will be applied when
     * generating each token.
     *
     * Value should be between 0 and 1. Set to false to disable XTC completely.
     */
    xtcProbability?: number | false;
    /**
     * Defines the lower probability threshold for the XTC (Exclude Top Choices) sampling technique.
     *
     * When XTC sampling is activated (based on xtcProbability), the algorithm identifies tokens with
     * probabilities between this threshold and 0.5, then removes all such tokens except the least
     * probable one. This helps introduce more diverse and unexpected tokens into the generation.
     *
     * Only takes effect when xtcProbability is enabled.
     */
    xtcThreshold?: number | false;
    /**
     * @deprecated We are still working on bringing logProbs to SDK. Stay tuned for updates.
     */
    logProbs?: number | false;
    /**
     * Specifies the number of CPU threads to allocate for model inference.
     *
     * Higher values can improve performance on multi-core systems but may compete with other
     * processes. For example, on an 8-core system, a value of 4-6 might provide good performance
     * while leaving resources for other tasks.
     *
     * If not specified, the system will use a default value based on available hardware.
     */
    cpuThreads?: number;
    /**
     * Defines a custom template for formatting prompts before sending them to the model.
     *
     * Prompt templates allow you to control exactly how conversations are formatted, including
     * system messages, user inputs, and assistant responses. This is particularly useful when
     * working with models that expect specific formatting conventions.
     *
     * Different models may have different optimal prompt templates, so this allows for
     * model-specific customization.
     *
     * @deprecated The current type for promptTemplate is not yet finalized. We are working on a new
     * type that will be more flexible and easier to use. Stay tuned for updates.
     */
    promptTemplate?: LLMPromptTemplate;
    /**
     * The draft model to use for speculative decoding. Speculative decoding is a technique that can
     * drastically increase the generation speed (up to 3x for larger models) by paring a main model
     * with a smaller draft model.
     *
     * See here for more information: https://lmstudio.ai/docs/advanced/speculative-decoding
     *
     * You do not need to load the draft model yourself. Simply specifying its model key here is
     * enough.
     */
    draftModel?: string;
    /**
     * Warning: Experimental and subject to change.
     *
     * @alpha
     * @deprecated This feature is experimental and may change or be removed in the future.
     */
    speculativeDecodingNumDraftTokensExact?: number;
    /**
     * Warning: Experimental and subject to change.
     *
     * Minimum number of drafted tokens required to run draft through the main model.
     *
     * @alpha
     *
     */
    speculativeDecodingMinDraftLengthToConsider?: number;
    /**
     * Warning: Experimental and subject to change.
     *
     * @alpha
     * @deprecated This feature is experimental and may change or be removed in the future.
     */
    speculativeDecodingMinContinueDraftingProbability?: number;
    /**
     * How to parse the reasoning sections in the model output. Only need to specify the `startString`
     * and the `endString`.
     *
     * For example, DeepSeek models use:
     *
     * ```
     * reasoningParsing: {
     *   enabled: true,
     *   startString: "<think>",
     *   endString: "</think>",
     * }
     * ```
     */
    reasoningParsing?: LLMReasoningParsing;
}

/**
 * Represents a fragment of a prediction from an LLM. Note that a fragment may contain multiple
 * tokens.
 *
 * @public
 */
export declare interface LLMPredictionFragment {
    /**
     * String content of the fragment.
     */
    content: string;
    /**
     * Number of tokens contains in this fragment. Note this value is not always accurate as tokens
     * may be split across fragments. However, over a period of time, the sum of token counts of
     * multiple fragments will be close to the actual token count. As such, this value can be
     * accumulated to provide a "live tokens count".
     */
    tokensCount: number;
    /**
     * Whether this fragment contains tokens from the draft model.
     */
    containsDrafted: boolean;
    /**
     * Type of reasoning for this fragment. See {@link LLMPredictionFragmentReasoningType} for more
     * info.
     */
    reasoningType: LLMPredictionFragmentReasoningType;
}

/**
 * Represents the type of this fragment in terms of reasoning.
 *
 * - `none`: Content outside of a reasoning block.
 * - `reasoning`: Content inside a reasoning block.
 * - `reasoningStartTag`: Start tag of a reasoning block.
 * - `reasoningEndTag`: End tag of a reasoning block.
 *
 * @public
 */
export declare type LLMPredictionFragmentReasoningType = "none" | "reasoning" | "reasoningStartTag" | "reasoningEndTag";

/**
 * A {@link LLMPredictionFragment} with the index of the prediction within `.act(...)`.
 *
 * See {@link LLMPredictionFragment} for more fields.
 *
 * @public
 */
export declare type LLMPredictionFragmentWithRoundIndex = LLMPredictionFragment & {
    roundIndex: number;
};

/**
 * Options for {@link LLMDynamicHandle#complete}.
 *
 * Note, this interface extends {@link LLMPredictionConfigInput}. See its documentation for more
 * fields.
 *
 * Alternatively, use your IDE/editor's intellisense to see the fields.
 *
 * @public
 */
export declare interface LLMPredictionOpts<TStructuredOutputType = unknown> extends LLMPredictionConfigInput<TStructuredOutputType> {
    /**
     * A callback that is called when the model is processing the prompt. The callback is called with
     * a number between 0 and 1, representing the progress of the prompt processing.
     *
     * Prompt processing progress callbacks will only be called before the first token is emitted.
     */
    onPromptProcessingProgress?: (progress: number) => void;
    /**
     * A callback that is called when the model has output the first token.
     */
    onFirstToken?: () => void;
    /**
     * A callback for each fragment that is output by the model.
     */
    onPredictionFragment?: (fragment: LLMPredictionFragment) => void;
    /**
     * An abort signal that can be used to cancel the prediction.
     */
    signal?: AbortSignal;
    /**
     * Which preset to use.
     *
     * @remarks
     *
     * This preset selection is "layered" between your overrides and the "server session" config.
     * Which means, other fields you specify in this opts object will override the preset, while the
     * preset content will override the "server session" config.
     */
    preset?: string;
}

/** @public */
export declare interface LLMPredictionStats {
    /**
     * The reason why the prediction stopped.
     *
     * This is a string enum with the following possible values:
     *
     * - `userStopped`: The user stopped the prediction. This includes calling the `cancel` method on
     *   the `OngoingPrediction` object.
     * - `modelUnloaded`: The model was unloaded during the prediction.
     * - `failed`: An error occurred during the prediction.
     * - `eosFound`: The model predicted an end-of-sequence token, which is a way for the model to
     *   indicate that it "thinks" the sequence is complete.
     * - `stopStringFound`: A stop string was found in the prediction. (Stop strings can be specified
     *   with the `stopStrings` config option. This stop reason will only occur if the `stopStrings`
     *   config option is set.)
     * - `maxPredictedTokensReached`: The maximum number of tokens to predict was reached. (Length
     *   limit can be specified with the `maxPredictedTokens` config option. This stop reason will
     *   only occur if the `maxPredictedTokens` config option is set to a value other than -1.)
     * - `contextLengthReached`: The context length was reached. This stop reason will only occur if
     *   the `contextOverflowPolicy` is set to `stopAtLimit`.
     */
    stopReason: LLMPredictionStopReason;
    /**
     * The average number of tokens predicted per second.
     *
     * Note: This value can be undefined in the case of a very short prediction which results in a
     * NaN or a Infinity value.
     */
    tokensPerSecond?: number;
    /**
     * The number of GPU layers used in the prediction. (Currently not correct.)
     */
    numGpuLayers?: number;
    /**
     * The time it took to predict the first token in seconds.
     */
    timeToFirstTokenSec?: number;
    /**
     * The number of tokens that were supplied.
     */
    promptTokensCount?: number;
    /**
     * The number of tokens that were predicted.
     */
    predictedTokensCount?: number;
    /**
     * The total number of tokens. This is the sum of the prompt tokens and the predicted tokens.
     */
    totalTokensCount?: number;
    /**
     * If the prediction used speculative decoding, this is the model key of the draft model that was
     * used.
     */
    usedDraftModelKey?: string;
    /**
     * Total number of tokens generated by the draft model when using speculative decoding. Undefined
     * if speculative decoding is not used.
     *
     * ```
     * totalDraftTokensCount =
     *   rejectedDraftTokensCount + acceptedDraftTokensCount + ignoredDraftTokensCount
     * ```
     */
    totalDraftTokensCount?: number;
    /**
     * Number of drafted tokens that are accepted by the main model. The higher the better. Undefined
     * if speculative decoding is not used.
     *
     * ```
     * totalDraftTokensCount =
     *   rejectedDraftTokensCount + acceptedDraftTokensCount + ignoredDraftTokensCount
     * ```
     */
    acceptedDraftTokensCount?: number;
    /**
     * Number of draft tokens that are rejected by the main model. The lower the better. Undefined if
     * speculative decoding is not used.
     *
     * ```
     * totalDraftTokensCount =
     *   rejectedDraftTokensCount + acceptedDraftTokensCount + ignoredDraftTokensCount
     * ```
     */
    rejectedDraftTokensCount?: number;
    /**
     * Number of draft tokens that were not sent to the main model for decoding. Undefined if
     * speculative decoding is not used.
     *
     * ```
     * totalDraftTokensCount =
     *   rejectedDraftTokensCount + acceptedDraftTokensCount + ignoredDraftTokensCount
     * ```
     */
    ignoredDraftTokensCount?: number;
}

/**
 * Represents the reason why a prediction stopped. Only the following values are possible:
 *
 * - `userStopped`: The user stopped the prediction. This includes calling the `cancel` method on
 *   the `OngoingPrediction` object.
 * - `modelUnloaded`: The model was unloaded during the prediction.
 * - `failed`: An error occurred during the prediction.
 * - `eosFound`: The model predicted an end-of-sequence token, which is a way for the model to
 *   indicate that it "thinks" the sequence is complete.
 * - `stopStringFound`: A stop string was found in the prediction. (Stop strings can be specified
 *   with the `stopStrings` config option. This stop reason will only occur if the `stopStrings`
 *   config option is set to an array of strings.)
 * - `maxPredictedTokensReached`: The maximum number of tokens to predict was reached. (Length limit
 *   can be specified with the `maxPredictedTokens` config option. This stop reason will only occur
 *   if the `maxPredictedTokens` config option is set to a value other than -1.)
 * - `contextLengthReached`: The context length was reached. This stop reason will only occur if the
 *   `contextOverflowPolicy` is set to `stopAtLimit`.
 *
 * @public
 */
export declare type LLMPredictionStopReason = "userStopped" | "modelUnloaded" | "failed" | "eosFound" | "stopStringFound" | "toolCalls" | "maxPredictedTokensReached" | "contextLengthReached";

/**
 * @public
 */
export declare interface LLMPromptTemplate {
    type: LLMPromptTemplateType;
    manualPromptTemplate?: LLMManualPromptTemplate;
    jinjaPromptTemplate?: LLMJinjaPromptTemplate;
    /**
     * Additional stop strings to be used with this template.
     */
    stopStrings: Array<string>;
}

/** @public */
export declare type LLMPromptTemplateType = "manual" | "jinja";

/**
 * How to parse reasoning sections in the model output. An easier to use type will be added in the
 * future.
 *
 * @public
 */
export declare interface LLMReasoningParsing {
    /**
     * Whether to enable reasoning parsing.
     */
    enabled: boolean;
    startString: string;
    endString: string;
}

/**
 * Options for {@link LLMDynamicHandle#respond}.
 *
 * Note, this interface extends {@link LLMPredictionOpts} and {@link LLMPredictionConfigInput}. See
 * their documentation for more fields.
 *
 * Alternatively, use your IDE/editor's intellisense to see the fields.
 *
 * @public
 */
export declare interface LLMRespondOpts<TStructuredOutputType = unknown> extends LLMPredictionOpts<TStructuredOutputType> {
    /**
     * A convenience callback that is called when the model finishes generation. The callback is
     * called with a message that has the role set to "assistant" and the content set to the generated
     * text.
     *
     * This callback is useful if you want to add the generated message to a chat.
     *
     * For example:
     *
     * ```ts
     * const chat = Chat.empty();
     * chat.append("user", "When will The Winds of Winter be released?");
     *
     * const llm = client.llm.model();
     * const prediction = llm.respond(chat, {
     *   onMessage: message => chat.append(message),
     * });
     * ```
     */
    onMessage?: (message: ChatMessage) => void;
}

/**
 * How to split the model across GPUs.
 * - "evenly": Splits model evenly across GPUs
 * - "favorMainGpu": Fill the main GPU first, then fill the rest of the GPUs evenly
 *
 * @public
 * @deprecated We are currently working on an improved way to control split. You can use this for
 * now. We will offer the alternative before this feature is removed.
 */
export declare type LLMSplitStrategy = "evenly" | "favorMainGpu";

/**
 * Settings for structured prediction. Structured prediction is a way to force the model to generate
 * predictions that conform to a specific structure.
 *
 * For example, you can use structured prediction to make the model only generate valid JSON, or
 * event JSON that conforms to a specific schema (i.e. having strict types).
 *
 * Some examples:
 *
 * Only generate valid JSON:
 *
 * ```ts
 * const prediction = model.complete("...", {
 *   maxTokens: 100,
 *   structured: { type: "json" },
 * });
 * ```
 *
 * Only generate JSON that conforms to a specific schema (See https://json-schema.org/ for more
 * information on authoring JSON schema):
 *
 * ```ts
 * const schema = {
 *   type: "object",
 *   properties: {
 *     name: { type: "string" },
 *     age: { type: "number" },
 *   },
 *   required: ["name", "age"],
 * };
 * const prediction = model.complete("...", {
 *   maxTokens: 100,
 *   structured: { type: "json", jsonSchema: schema },
 * });
 * ```
 *
 * By default, `{ type: "none" }` is used, which means no structured prediction is used.
 *
 * Caveats:
 *
 * - Although the model is forced to generate predictions that conform to the specified structure,
 *   the prediction may be interrupted (for example, if the user stops the prediction). When that
 *   happens, the partial result may not conform to the specified structure. Thus, always check the
 *   prediction result before using it, for example, by wrapping the `JSON.parse` inside a try-catch
 *   block.
 * - In certain cases, the model may get stuck. For example, when forcing it to generate valid JSON,
 *   it may generate a opening brace `{` but never generate a closing brace `}`. In such cases, the
 *   prediction will go on forever until the context length is reached, which can take a long time.
 *   Therefore, it is recommended to always set a `maxTokens` limit.
 *
 * @public
 */
export declare type LLMStructuredPredictionSetting = {
    type: LLMStructuredPredictionType;
    jsonSchema?: any;
    gbnfGrammar?: string;
};

/**
 * @public
 */
export declare type LLMStructuredPredictionType = "none" | "json" | "gbnf";

/**
 * TODO: Documentation
 *
 * @public
 */
export declare type LLMTool = {
    type: "function";
    function: {
        name: string;
        description?: string;
        parameters?: LLMToolParameters;
    };
};

/**
 * TODO: Documentation
 *
 * @public
 */
export declare type LLMToolParameters = {
    type: "object";
    properties: Record<string, any>;
    required?: string[];
    additionalProperties?: boolean;
};

/**
 * TODO: Documentation
 *
 * @public
 */
export declare type LLMToolUseSetting = {
    type: "none";
} | {
    type: "toolArray";
    tools?: LLMTool[];
};

/** @public */
export declare class LMStudioClient {
    readonly clientIdentifier: string;
    readonly llm: LLMNamespace;
    readonly embedding: EmbeddingNamespace;
    readonly system: SystemNamespace;
    readonly diagnostics: DiagnosticsNamespace;
    readonly files: FilesNamespace;
    readonly repository: RepositoryNamespace;
    /**
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    readonly plugins: PluginsNamespace;
    private isLocalhostWithGivenPortLMStudioServer;
    /**
     * Guess the base URL of the LM Studio server by visiting localhost on various default ports.
     */
    private guessBaseUrl;
    private createPort;
    private resolvingBaseUrl;
    private verboseErrorMessages;
    constructor(opts?: LMStudioClientConstructorOpts);
}

/** @public */
export declare interface LMStudioClientConstructorOpts {
    /**
     * Changes the logger that is used by LMStudioClient internally. The default logger is `console`.
     * By default, LMStudioClient only logs warnings and errors that require user intervention. If the
     * `verbose` option is enabled while calling supporting methods, those messages will also be
     * directed to the specified logger.
     */
    logger?: LoggerInterface;
    /**
     * The base URL of the LM Studio server. If not provided, LM Studio will attempt to connect to the
     * localhost with various default ports.
     *
     * If you have set a custom port and/or are reverse-proxying, you should pass in the baseUrl.
     *
     * Since LM Studio uses WebSockets, the protocol must be "ws" or "wss".
     *
     * For example, if have changed the port to 8080, you should create the LMStudioClient like so:
     *
     * ```typescript
     * const client = new LMStudioClient({ baseUrl: "ws://127.0.0.1:8080" });
     * ```
     */
    baseUrl?: string;
    /**
     * Whether to include stack traces in the errors caused by LM Studio. By default, this is set to
     * `false`. If set to `true`, LM Studio SDK will include a stack trace in the error message.
     */
    verboseErrorMessages?: boolean;
    /**
     * Changes the client identifier used to authenticate with LM Studio. By default, it uses a
     * randomly generated string.
     *
     * If you wish to share resources across multiple LMStudioClient, you should set them to use the
     * same `clientIdentifier` and `clientPasskey`.
     */
    clientIdentifier?: string;
    /**
     * Changes the client passkey used to authenticate with LM Studio. By default, it uses a randomly
     * generated string.
     *
     * If you wish to share resources across multiple LMStudioClient, you should set them to use the
     * same `clientIdentifier` and `clientPasskey`.
     */
    clientPasskey?: string;
}

/** @public */
export declare interface LoggerInterface {
    info(...messages: Array<unknown>): void;
    error(...messages: Array<unknown>): void;
    warn(...messages: Array<unknown>): void;
    debug(...messages: Array<unknown>): void;
}

/** @public */
export declare type LogLevel = "debug" | "info" | "warn" | "error";

/**
 * Represents some underlying data that may or may not be mutable.
 *
 * @public
 */
export declare abstract class MaybeMutable<Data> {
    protected readonly data: Data;
    protected readonly mutable: boolean;
    protected constructor(data: Data, mutable: boolean);
    /**
     * Gets the class name. This is used for printing errors.
     */
    protected abstract getClassName(): string;
    /**
     * Creates a new instance of the class with the given data.
     */
    protected abstract create(data: Data, mutable: boolean): this;
    /**
     * Clones the data.
     */
    protected abstract cloneData(data: Data): Data;
    asMutableCopy(): this;
    asImmutableCopy(): this;
    protected guardMutable(): void;
}

/**
 * @public
 */
export declare type ModelCompatibilityType = "gguf" | "safetensors" | "onnx" | "ggml" | "mlx_placeholder" | "torch_safetensors";

/**
 * @public
 */
export declare type ModelDomainType = "llm" | "embedding" | "imageGen" | "transcription" | "tts";

/**
 * Information about a model.
 *
 * @public
 */
export declare type ModelInfo = LLMInfo | EmbeddingModelInfo;

/**
 * Represents info of a model that is downloaded and sits on the disk. This is the base type shared
 * by all models of different domains.
 *
 * @public
 */
export declare interface ModelInfoBase {
    /**
     * The key of the model. Use to load the model.
     */
    modelKey: string;
    /**
     * The format of the model.
     */
    format: ModelCompatibilityType;
    /**
     * Machine generated name of the model.
     */
    displayName: string;
    /**
     * The relative path of the model.
     */
    path: string;
    /**
     * The size of the model in bytes.
     */
    sizeBytes: number;
    /**
     * A string that represents the number of params in the model. May not always be available.
     */
    paramsString?: string;
    /**
     * The architecture of the model.
     */
    architecture?: string;
}

/**
 * Information about a model that is loaded.
 *
 * @public
 */
export declare type ModelInstanceInfo = LLMInstanceInfo | EmbeddingModelInstanceInfo;

/**
 * Represents info of a model that is already loaded. Contains all fields from
 * {@link ModelInfoBase}. This is the base typed share by all model instances of different domains.
 *
 * @public
 */
export declare interface ModelInstanceInfoBase extends ModelInfoBase {
    /**
     * The identifier of the instance.
     */
    identifier: string;
    /**
     * The internal immutable reference of the instance.
     */
    instanceReference: string;
}

/**
 * Abstract namespace for namespaces that deal with models.
 *
 * @public
 */
export declare abstract class ModelNamespace<TLoadModelConfig, TModelInstanceInfo extends ModelInstanceInfoBase, TModelInfo extends ModelInfoBase, TDynamicHandle extends DynamicHandle<TModelInstanceInfo>, TSpecificModel> {
    /**
     * Load a model for inferencing. The first parameter is the model key. The second parameter is an
     * optional object with additional options.
     *
     * To find out what models are available, you can use the `lms ls` command, or programmatically
     * use the `client.system.listDownloadedModels` method.
     *
     * Here are some examples:
     *
     * Loading Llama 3.2:
     *
     * ```typescript
     * const model = await client.llm.load("llama-3.2-3b-instruct");
     * ```
     *
     * Once loaded, see {@link LLMDynamicHandle} or {@link EmbeddingDynamicHandle} for how to use the
     * model for inferencing or other things you can do with the model.
     *
     * @param modelKey - The path of the model to load.
     * @param opts - Options for loading the model.
     * @returns A promise that resolves to the model that can be used for inferencing
     */
    load(modelKey: string, opts?: BaseLoadModelOpts<TLoadModelConfig>): Promise<TSpecificModel>;
    /**
     * Unload a model. Once a model is unloaded, it can no longer be used. If you wish to use the
     * model afterwards, you will need to load it with {@link LLMNamespace#loadModel} again.
     *
     * @param identifier - The identifier of the model to unload.
     */
    unload(identifier: string): Promise<void>;
    /**
     * List all the currently loaded models.
     */
    listLoaded(): Promise<Array<TSpecificModel>>;
    /**
     * Get any loaded model of this domain.
     */
    private getAny;
    /**
     * Get a dynamic model handle for any loaded model that satisfies the given query.
     *
     * For more information on the query, see {@link ModelQuery}.
     *
     * Note: The returned handle is not tied to any specific loaded model. Instead, it represents a
     * "handle for a model that satisfies the given query". If the model that satisfies the query is
     * unloaded, the handle will still be valid, but any method calls on it will fail. And later, if a
     * new model is loaded that satisfies the query, the handle will be usable again.
     *
     * You can use {@link DynamicHandle#getModelInfo} to get information about the model that is
     * currently associated with this handle.
     *
     * @example
     *
     * If you have loaded a model with the identifier "my-model", you can use it like this:
     *
     * ```ts
     * const dh = client.llm.createDynamicHandle({ identifier: "my-model" });
     * const prediction = dh.complete("...");
     * ```
     *
     * @example
     *
     * Use the Gemma 2B IT model (given it is already loaded elsewhere):
     *
     * ```ts
     * const dh = client.llm.createDynamicHandle({ path: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF" });
     * const prediction = dh.complete("...");
     * ```
     *
     * @param query - The query to use to get the model.
     */
    createDynamicHandle(query: ModelQuery): TDynamicHandle;
    /**
     * Get a dynamic model handle by its identifier.
     *
     * Note: The returned handle is not tied to any specific loaded model. Instead, it represents a
     * "handle for a model with the given identifier". If the model with the given identifier is
     * unloaded, the handle will still be valid, but any method calls on it will fail. And later, if a
     * new model is loaded with the same identifier, the handle will be usable again.
     *
     * You can use {@link DynamicHandle#getModelInfo} to get information about the model that is
     * currently associated with this handle.
     *
     * @example
     *
     * If you have loaded a model with the identifier "my-model", you can get use it like this:
     *
     * ```ts
     * const dh = client.llm.createDynamicHandle("my-model");
     * const prediction = dh.complete("...");
     * ```
     *
     * @param identifier - The identifier of the model to get.
     */
    createDynamicHandle(identifier: string): TDynamicHandle;
    /**
     * Create a dynamic handle from the internal instance reference.
     *
     * @alpha
     */
    createDynamicHandleFromInstanceReference(instanceReference: string): TDynamicHandle;
    /**
     * Get a model by its identifier. If no model is loaded with such identifier, load a model with
     * the given key. This is the recommended way of getting a model to work with.
     *
     * For example, to use the DeepSeek r1 distill of Llama 8B:
     *
     * ```typescript
     * const model = await client.llm.model("deepseek-r1-distill-llama-8b");
     * ```
     */
    model(modelKey: string, opts?: BaseLoadModelOpts<TLoadModelConfig>): Promise<TSpecificModel>;
    /**
     * Get any loaded model of this domain. If you want to use a specific model, pass in the model key
     * as a parameter.
     */
    model(): Promise<TSpecificModel>;
}

/**
 * Represents a query for a loaded LLM.
 *
 * @public
 */
export declare interface ModelQuery {
    /**
     * The domain of the model.
     */
    domain?: ModelDomainType;
    /**
     * If specified, the model must have exactly this identifier.
     *
     * Note: The identifier of a model is set when loading the model. It defaults to the filename of
     * the model if not specified. However, this default behavior should not be relied upon. If you
     * wish to query a model by its path, you should specify the path instead of the identifier:
     *
     * Instead of
     *
     * ```ts
     * const model = client.llm.get({ identifier: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF" });
     * // OR
     * const model = client.llm.get("lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF");
     * ```
     *
     * Use
     *
     * ```ts
     * const model = client.llm.get({ path: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF" });
     * ```
     */
    identifier?: string;
    /**
     * If specified, the model must have this path.
     *
     * When specifying the model path, you can use the following format:
     *
     * `<publisher>/<repo>[/model_file]`
     *
     * If `model_file` is not specified, any quantization of the model will match this query.
     *
     * Here are some examples:
     *
     * Query any loaded Llama 3 model:
     *
     * ```ts
     * const model = client.llm.get({
     *   path: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF",
     * });
     * ```
     *
     * Query any loaded model with a specific quantization of the Llama 3 model:
     *
     * ```ts
     * const model = client.llm.get({
     *   path: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
     * });
     * ```
     */
    path?: string;
    /**
     * If true, the model must have vision capabilities. If false, the model must not have vision
     * capabilities.
     */
    vision?: boolean;
}

/** @public */
export declare interface ModelSearchOpts {
    /**
     * The search term to use when searching for models. If not provided, recommended models will
     * be returned.
     */
    searchTerm?: string;
    /**
     * How many results to return. If not provided, this value will be decided by LM Studio.
     */
    limit?: number;
    /**
     * The model compatibility types to filter by. If not provided, only models that are supported
     * by your current runtimes will be returned.
     */
    compatibilityTypes?: Array<ModelCompatibilityType>;
}

/** @public */
export declare class ModelSearchResultDownloadOption {
    private readonly logger;
    private readonly data;
    readonly quantization?: string;
    readonly name: string;
    readonly sizeBytes: number;
    readonly fitEstimation?: ModelSearchResultDownloadOptionFitEstimation;
    readonly indexedModelIdentifier: string;
    isRecommended(): boolean;
    /**
     * Download the model. Returns the model key which can be used to load the model.
     */
    download(opts?: DownloadOpts): Promise<string>;
}

/**
 * @public
 */
export declare type ModelSearchResultDownloadOptionFitEstimation = "fullGPUOffload" | "partialGPUOffload" | "fitWithoutGPU" | "willNotFit";

/** @public */
export declare class ModelSearchResultEntry {
    private readonly logger;
    private readonly data;
    readonly name: string;
    isExactMatch(): boolean;
    isStaffPick(): boolean;
    getDownloadOptions(): Promise<Array<ModelSearchResultDownloadOption>>;
}

/**
 * Represents an ongoing prediction.
 *
 * Note, this class is Promise-like, meaning you can use it as a promise. It resolves to a
 * {@link PredictionResult}, which contains the generated text in the `.content` property. Example
 * usage:
 *
 * ```typescript
 * const result = await model.complete("When will The Winds of Winter be released?");
 * console.log(result.content);
 * ```
 *
 * Or you can use instances methods like `then` and `catch` to handle the result or error of the
 * prediction.
 *
 * ```typescript
 * model.complete("When will The Winds of Winter be released?")
 *  .then(result =\> console.log(result.content))
 *  .catch(error =\> console.error(error));
 * ```
 *
 * Alternatively, you can also stream the result (process the results as more content is being
 * generated). For example:
 *
 * ```typescript
 * for await (const { content } of model.complete("When will The Winds of Winter be released?")) {
 *   process.stdout.write(content);
 * }
 * ```
 *
 * @public
 */
export declare class OngoingPrediction<TStructuredOutputType = unknown> extends StreamablePromise<LLMPredictionFragment, unknown extends TStructuredOutputType ? PredictionResult : StructuredPredictionResult<TStructuredOutputType>> {
    private readonly onCancel;
    private readonly parser;
    private stats;
    private modelInfo;
    private loadModelConfig;
    private predictionConfig;
    protected collect(fragments: ReadonlyArray<LLMPredictionFragment>): Promise<any>;
    private constructor();
    /**
     * Get the final prediction results. If you have been streaming the results, awaiting on this
     * method will take no extra effort, as the results are already available in the internal buffer.
     *
     * Example:
     *
     * ```typescript
     * const prediction = model.complete("When will The Winds of Winter be released?");
     * for await (const { content } of prediction) {
     *   process.stdout.write(content);
     * }
     * const result = await prediction.result();
     * console.log(result.stats);
     * ```
     *
     * Technically, awaiting on this method is the same as awaiting on the instance itself:
     *
     * ```typescript
     * await prediction.result();
     *
     * // Is the same as:
     *
     * await prediction;
     * ```
     */
    result(): Promise<unknown extends TStructuredOutputType ? PredictionResult : StructuredPredictionResult<TStructuredOutputType>>;
    /**
     * Cancels the prediction. This will stop the prediction with stop reason `userStopped`. See
     * {@link LLMPredictionStopReason} for other reasons that a prediction might stop.
     */
    cancel(): Promise<void>;
}

/**
 * @public
 */
export declare interface ParsedConfig<TVirtualConfigSchematics extends VirtualConfigSchematics> {
    [configSchematicsBrand]?: TVirtualConfigSchematics;
    get<TKey extends keyof TVirtualConfigSchematics & string>(key: TKey): TVirtualConfigSchematics[TKey]["type"];
}

/**
 * @public
 */
export declare interface PluginContext {
    /**
     * Sets the config schematics associated with this plugin context. Returns the same PluginContext.
     */
    withConfigSchematics: (configSchematics: ConfigSchematics<VirtualConfigSchematics>) => PluginContext;
    /**
     * Sets the generator associated with this plugin context. Returns the same PluginContext.
     */
    withGenerator(generate: Generator_2): PluginContext;
    /**
     * Sets the preprocessor associated with this plugin context. Returns the same PluginContext.
     */
    withPreprocessor(preprocess: Preprocessor): PluginContext;
}

/**
 * @public
 */
export declare interface PluginManifest extends ArtifactManifestBase {
    type: "plugin";
    runner: PluginRunnerType;
}

/**
 * @public
 */
export declare type PluginRunnerType = "ecmascript";

/**
 * @public
 *
 * The namespace for file-related operations. Currently no public-facing methods.
 */
export declare class PluginsNamespace {
    private readonly client;
    private readonly validator;
    private readonly rootLogger;
    /**
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    registerDevelopmentPlugin(opts: RegisterDevelopmentPluginOpts): Promise<RegisterDevelopmentPluginResult>;
    /**
     * Requests LM Studio to reindex all the plugins.
     *
     * CAVEAT: Currently, we do not wait for the reindex to complete before returning. In the future,
     * we will change this behavior and only return after the reindex is completed.
     *
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    reindexPlugins(): Promise<void>;
    /**
     * Sets the preprocessor to be used by the plugin represented by this client.
     *
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    setPreprocessor(preprocessor: Preprocessor): void;
    /**
     * Sets the preprocessor to be used by the plugin represented by this client.
     *
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    setGenerator(generator: Generator_2): void;
    /**
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    setConfigSchematics(configSchematics: ConfigSchematics<any>): Promise<void>;
    /**
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    initCompleted(): Promise<void>;
}

/**
 * Controller for a citation block in the prediction process. Currently cannot do anything.
 *
 * @public
 */
export declare class PredictionProcessCitationBlockController {
    private readonly id;
}

/**
 * @public
 *
 * TODO: Documentation
 */
export declare class PredictionProcessContentBlockController {
    private readonly id;
    appendText(text: string, { tokensCount, fromDraftModel }?: ContentBlockAppendTextOpts): void;
    replaceText(text: string): void;
    setStyle(style: ContentBlockStyle): void;
    setPrefix(prefix: string): void;
    setSuffix(suffix: string): void;
    attachGenInfo(genInfo: LLMGenInfo): void;
    pipeFrom(prediction: OngoingPrediction): Promise<PredictionResult>;
}

/**
 * Controller for a debug info block in the prediction process. Currently cannot do anything.
 *
 * @public
 */
export declare class PredictionProcessDebugInfoBlockController {
    private readonly id;
}

/**
 * Controller for a status block in the prediction process.
 *
 * @public
 */
export declare class PredictionProcessStatusController {
    private readonly id;
    private readonly indentation;
    private lastSubStatus;
    private lastState;
    setText(text: string): void;
    setState(state: StatusStepState): void;
    remove(): void;
    private getNestedLastSubStatusBlockId;
    addSubStatus(initialState: StatusStepState): PredictionProcessStatusController;
}

/**
 * Represents the result of a prediction.
 *
 * The most notably property is {@link PredictionResult#content}, which contains the generated text.
 * Additionally, the {@link PredictionResult#stats} property contains statistics about the
 * prediction.
 *
 * @public
 */
export declare class PredictionResult {
    /**
     * The newly generated text as predicted by the LLM.
     */
    readonly content: string;
    /**
     * Part of the generated text that is "reasoning" content. For example, text inside <think>
     * tags. You can adjust what is considered reasoning content by changing the `reasoningParsing`
     * field when performing the prediction.
     *
     * @experimental The name of this field may change in the future.
     */
    readonly reasoningContent: string;
    /**
     * Part of the generated that is not "reasoning" content. For example, text outside <think>
     * tags. You can adjust what is considered reasoning content by changing the `reasoningParsing`
     * field when performing the prediction.
     *
     * @experimental The name of this field may change in the future.
     */
    readonly nonReasoningContent: string;
    /**
     * Statistics about the prediction.
     */
    readonly stats: LLMPredictionStats;
    /**
     * Information about the model used for the prediction.
     */
    readonly modelInfo: LLMInstanceInfo;
    /**
     * The 0-indexed round index of the prediction in multi-round scenario (for example,
     * `.act`). Will always be 0 for single-round predictions such as `.respond` or `.complete`.
     */
    readonly roundIndex: number;
    /**
     * The configuration used to load the model. Not stable, subject to change.
     *
     * @deprecated Not stable - subject to change
     */
    readonly loadConfig: KVConfig;
    /**
     * The configuration used for the prediction. Not stable, subject to change.
     *
     * @deprecated Not stable - subject to change
     */
    readonly predictionConfig: KVConfig;
    constructor(
    /**
     * The newly generated text as predicted by the LLM.
     */
    content: string, 
    /**
     * Part of the generated text that is "reasoning" content. For example, text inside <think>
     * tags. You can adjust what is considered reasoning content by changing the `reasoningParsing`
     * field when performing the prediction.
     *
     * @experimental The name of this field may change in the future.
     */
    reasoningContent: string, 
    /**
     * Part of the generated that is not "reasoning" content. For example, text outside <think>
     * tags. You can adjust what is considered reasoning content by changing the `reasoningParsing`
     * field when performing the prediction.
     *
     * @experimental The name of this field may change in the future.
     */
    nonReasoningContent: string, 
    /**
     * Statistics about the prediction.
     */
    stats: LLMPredictionStats, 
    /**
     * Information about the model used for the prediction.
     */
    modelInfo: LLMInstanceInfo, 
    /**
     * The 0-indexed round index of the prediction in multi-round scenario (for example,
     * `.act`). Will always be 0 for single-round predictions such as `.respond` or `.complete`.
     */
    roundIndex: number, 
    /**
     * The configuration used to load the model. Not stable, subject to change.
     *
     * @deprecated Not stable - subject to change
     */
    loadConfig: KVConfig, 
    /**
     * The configuration used for the prediction. Not stable, subject to change.
     *
     * @deprecated Not stable - subject to change
     */
    predictionConfig: KVConfig);
}

/**
 * TODO: Documentation
 *
 * @public
 */
export declare type Preprocessor = (ctl: PreprocessorController, userMessage: ChatMessage) => Promise<string | ChatMessage>;

/**
 * @public
 */
export declare type PreprocessorController = Omit<ProcessingController, "createContentBlock" | "setSenderName">;

/**
 * @public
 */
export declare class ProcessingController {
    readonly client: LMStudioClient;
    readonly abortSignal: AbortSignal;
    private sendUpdate;
    getPluginConfig<TVirtualConfigSchematics extends VirtualConfigSchematics>(configSchematics: ConfigSchematics<TVirtualConfigSchematics>): ParsedConfig<TVirtualConfigSchematics>;
    /**
     * Gets a mutable copy of the current history. The returned history is a copy, so mutating it will
     * not affect the actual history. It is mutable for convenience reasons.
     *
     * - If you are a preprocessor, this will not include the user message you are currently
     *   preprocessing.
     * - If you are a generator, this will include the user message, and can be fed into the
     *   {@link LLMDynamicHandle#respond} directly.
     */
    pullHistory(): Promise<Chat>;
    createStatus(initialState: StatusStepState): PredictionProcessStatusController;
    addCitations(retrievalResult: RetrievalResult): void;
    addCitations(entries: Array<RetrievalResultEntry>): void;
    createCitationBlock(citedText: string, source: CreateCitationBlockOpts): PredictionProcessCitationBlockController;
    createContentBlock({ includeInContext, style, prefix, suffix, }?: CreateContentBlockOpts): PredictionProcessContentBlockController;
    debug(...messages: Array<any>): void;
    getPredictionConfig(): LLMPredictionConfig;
    readonly model: Readonly<{
        getOrLoad: () => Promise<LLM>;
    }>;
    /**
     * Sets the sender name for this message. The sender name shown above the message in the chat.
     */
    setSenderName(name: string): Promise<void>;
    /**
     * Throws an error if the prediction process has been aborted. Sprinkle this throughout your code
     * to ensure that the prediction process is aborted as soon as possible.
     */
    guardAbort(): void;
    /**
     * Whether this prediction process has had any status.
     */
    hasStatus(): Promise<boolean>;
    /**
     * Returns whether this conversation needs a name.
     */
    needsNaming(): Promise<boolean>;
    /**
     * Suggests a name for this conversation.
     */
    suggestName(name: string): Promise<void>;
}

/**
 * Options to use with {@link RepositoryNamespace#pushArtifact}.
 *
 * @public
 */
export declare interface PushArtifactOpts {
    path: string;
    onMessage?: (message: string) => void;
}

/**
 * Options to use with {@link PluginsNamespace#registerDevelopmentPlugin}.
 *
 * @public
 */
export declare interface RegisterDevelopmentPluginOpts {
    manifest: PluginManifest;
}

/**
 * Result of {@link PluginsNamespace#registerDevelopmentPlugin}.
 *
 * @public
 */
export declare interface RegisterDevelopmentPluginResult {
    clientIdentifier: string;
    clientPasskey: string;
    unregister: () => Promise<void>;
}

/** @public */
export declare class RepositoryNamespace {
    private readonly repositoryPort;
    private readonly validator;
    searchModels(opts: ModelSearchOpts): Promise<Array<ModelSearchResultEntry>>;
    /**
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    installPluginDependencies(pluginFolder: string): Promise<void>;
    /**
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    downloadArtifact(opts: DownloadArtifactOpts): Promise<void>;
    /**
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    pushArtifact(opts: PushArtifactOpts): Promise<void>;
    /**
     * @deprecated Plugin support is still in development. Stay tuned for updates.
     */
    ensureAuthenticated(opts: EnsureAuthenticatedOpts): Promise<void>;
}

/**
 * @public
 */
export declare interface RetrievalCallbacks {
    /**
     * Callback when the list of files to process is available. This list can be shorter than the list
     * passed in because some files may already have cached embeddings.
     *
     * @param filePathsToProcess - The list of files that will be processed.
     */
    onFileProcessList?: (filesToProcess: Array<FileHandle>) => void;
    /**
     * Callback when starting to process a file.
     *
     * @param file - The file being processed.
     * @param index - The index of the file in the list of files to process.
     * @param filePathsToProcess - The list of files that will be processed. This will be the same as
     * the list passed to `onFileProcessList`.
     */
    onFileProcessingStart?: (file: FileHandle, index: number, filesToProcess: Array<FileHandle>) => void;
    /**
     * Callback when processing a file has ended.
     *
     * @param file - The file that has been processed.
     * @param index - The index of the file in the list of files to process.
     * @param filePathsToProcess - The list of files that will be processed. This will be the same as
     * the list passed to `onFileProcessList`.
     */
    onFileProcessingEnd?: (file: FileHandle, index: number, filesToProcess: Array<FileHandle>) => void;
    /**
     * Callback when starting a processing step for a file. LM Studio process files one at a time and
     * processing each file involves multiple steps. This callback is called when starting a step.
     *
     * @param file - The file being processed.
     * @param step - The step being started.
     */
    onFileProcessingStepStart?: (file: FileHandle, step: RetrievalFileProcessingStep) => void;
    /**
     * Granular progress callback for a processing step.
     *
     * @param file - The file being processed.
     * @param step - The step being started.
     * @param progressInStep - The progress in the step for the step. This value is between 0 and 1.
     */
    onFileProcessingStepProgress?: (file: FileHandle, step: RetrievalFileProcessingStep, progressInStep: number) => void;
    /**
     * Callback when a processing step has ended.
     *
     * @param file - The file being processed.
     * @param step - The step that has ended.
     */
    onFileProcessingStepEnd?: (file: FileHandle, step: RetrievalFileProcessingStep) => void;
    /**
     * Callback when we have embedded all the files and are starting to search in the vector database.
     */
    onSearchingStart?: () => void;
    /**
     * Callback when we have finished searching in the vector database. The chunk usually will be
     * returned immediately after this callback.
     */
    onSearchingEnd?: () => void;
    /**
     * Controls the logging of retrieval progress.
     *
     * - If set to `true`, logs progress at the "info" level.
     * - If set to `false`, no logs are emitted. This is the default.
     * - If a specific logging level is desired, it can be provided as a string. Acceptable values are
     *   "debug", "info", "warn", and "error".
     *
     * Logs are directed to the logger specified during the `LMStudioClient` construction.
     *
     * Progress logs will be disabled if any of the callbacks are provided.
     *
     * Default value is "info", which logs progress at the "info" level.
     */
    verbose?: boolean | LogLevel;
}

/**
 * @public
 */
export declare interface RetrievalChunk {
    content: string;
    score: number;
    citation: CitationSource;
}

/**
 * @public
 */
export declare type RetrievalChunkingMethod = {
    type: "recursive-v1";
    chunkSize: number;
    chunkOverlap: number;
};

/**
 * @public
 */
export declare type RetrievalFileProcessingStep = "loading" | "chunking" | "embedding";

/**
 * @public
 * N.B.: onProgress returns progress as a float taking values from 0 to 1, 1 being completed
 */
export declare type RetrievalOpts = RetrievalCallbacks & {
    /**
     * The chunking method to use. By default uses recursive-v1 with chunk size 512 and chunk overlap
     * 100.
     */
    chunkingMethod?: RetrievalChunkingMethod;
    /**
     * The number of results to return.
     */
    limit?: number;
    /**
     * The embedding model to use.
     */
    embeddingModel?: EmbeddingDynamicHandle;
    /**
     * The path to the database.
     */
    databasePath?: string;
    /**
     * The signal to abort the retrieval
     */
    signal?: AbortSignal;
};

/** @public */
export declare interface RetrievalResult {
    entries: Array<RetrievalResultEntry>;
}

/** @public */
export declare interface RetrievalResultEntry {
    content: string;
    score: number;
    source: FileHandle;
}

/**
 * @public
 */
export declare interface SpecificModel extends DynamicHandle<ModelInstanceInfoBase> {
    readonly identifier: string;
    readonly path: string;
    unload(): Promise<void>;
}

/**
 * @public
 */
export declare interface StatusStepState {
    status: StatusStepStatus;
    text: string;
}

/**
 * @public
 */
export declare type StatusStepStatus = "waiting" | "loading" | "done" | "error" | "canceled";

/**
 * A StreamablePromise is a promise-like that is also async iterable. This means you can use it as a
 * promise (awaiting it, using `.then`, `.catch`, etc.), and you can also use it as an async
 * iterable (using `for await`).
 *
 * Notably, as much as it implements the async iterable interface, it is not a traditional iterable,
 * as it internally maintains a buffer and new values are pushed into the buffer by the producer, as
 * oppose to being pulled by the consumer.
 *
 * The async iterable interface is used instead of the Node.js object stream because streams are too
 * clunky to use, and the `for await` syntax is much more ergonomic for most people.
 *
 * If any iterator is created for this instance, an empty rejection handler will be attached to the
 * promise to prevent unhandled rejection warnings.
 *
 * This class is provided as an abstract class and is meant to be extended. Crucially, the `collect`
 * method must be implemented, which will be called to convert an array of values into the final
 * resolved value of the promise.
 *
 * In addition, the constructor of the subclass should be marked as private, and a static method
 * that exposes the constructor, the `finished` method, and the `push` method should be provided.
 *
 * @typeParam TFragment - The type of the individual fragments that are pushed into the buffer.
 * @typeParam TFinal - The type of the final resolved value of the promise.
 * @public
 */
export declare abstract class StreamablePromise<TFragment, TFinal> implements Promise<TFinal>, AsyncIterable<TFragment> {
    protected abstract collect(fragments: ReadonlyArray<TFragment>): Promise<TFinal>;
    private promiseFinal;
    private resolveFinal;
    private rejectFinal;
    protected status: "pending" | "resolved" | "rejected";
    private buffer;
    private nextFragmentPromiseBundle;
    /**
     * If there has ever been any iterators created for this instance. Once any iterator is created,
     * a reject handler will be attached to the promise to prevent unhandled rejection warnings, as
     * the errors will be handled by the iterator.
     *
     * The purpose of this variable is to prevent registering the reject handler more than once.
     */
    private hasIterator;
    /**
     * Called by the producer when it has finished producing values. If an error is provided, the
     * promise will be rejected with that error. If no error is provided, the promise will be resolved
     * with the final value.
     *
     * This method should be exposed in the static constructor of the subclass.
     *
     * @param error - The error to reject the promise with, if any.
     */
    protected finished(error?: any): void;
    /**
     * Called by the producer to push a new fragment into the buffer. This method should be exposed in
     * the static constructor of the subclass.
     *
     * This method should be exposed in the static constructor of the subclass.
     *
     * @param fragment - The fragment to push into the buffer.
     */
    protected push(fragment: TFragment): void;
    protected constructor();
    then<TResult1 = TFinal, TResult2 = never>(onfulfilled?: ((value: TFinal) => TResult1 | PromiseLike<TResult1>) | null | undefined, onrejected?: ((reason: any) => TResult2 | PromiseLike<TResult2>) | null | undefined): Promise<TResult1 | TResult2>;
    catch<TResult = never>(onrejected?: ((reason: any) => TResult | PromiseLike<TResult>) | null | undefined): Promise<TFinal | TResult>;
    finally(onfinally?: (() => void) | null | undefined): Promise<TFinal>;
    [Symbol.toStringTag]: string;
    /**
     * If nextFragmentPromiseBundle exists, it is returned. Otherwise, a new one is created and
     * returned.
     */
    private obtainNextFragmentPromiseBundle;
    [Symbol.asyncIterator](): AsyncIterator<TFragment, any, undefined>;
}

/**
 * Result of a typed structured prediction. In addition to a regular {@link PredictionResult}, there
 * is one additional field: {@link StructuredPredictionResult#parsed}.
 *
 * To enable typed structured prediction, you should pass in a zod schema as the structured option
 * when constructing the prediction config.
 *
 * @public
 */
export declare class StructuredPredictionResult<TStructuredOutputType = unknown> extends PredictionResult {
    /**
     * Parsed result of the structured output.
     */
    readonly parsed: TStructuredOutputType;
    constructor(content: string, reasoningContent: string, nonReasoningContent: string, stats: LLMPredictionStats, modelInfo: LLMInstanceInfo, roundIndex: number, loadConfig: KVConfig, predictionConfig: KVConfig, 
    /**
     * Parsed result of the structured output.
     */
    parsed: TStructuredOutputType);
}

/** @public */
export declare class SystemNamespace {
    private readonly systemPort;
    private readonly validator;
    /**
     * List all downloaded models.
     * @public
     */
    listDownloadedModels(): Promise<Array<ModelInfo>>;
    listDownloadedModels(domain: "llm"): Promise<Array<LLMInfo>>;
    listDownloadedModels(domain: "embedding"): Promise<Array<EmbeddingModelInfo>>;
    whenDisconnected(): Promise<void>;
    notify(notification: BackendNotification): Promise<void>;
    getLMStudioVersion(): Promise<{
        version: string;
        build: number;
    }>;
    /**
     * Sets an experiment flags for LM Studio. This is an unstable API and may change without notice.
     *
     * @experimental
     */
    unstable_setExperimentFlag(flag: string, value: boolean): Promise<void>;
    /**
     * Gets all experiment flags for LM Studio. This is an unstable API and may change without notice.
     *
     * @experimental
     */
    unstable_getExperimentFlags(): Promise<Array<string>>;
}

/**
 * Represents a tool that can be given to an LLM with `.act`.
 *
 * @public
 */
export declare type Tool = FunctionTool;

/**
 * A function that can be used to create a function `Tool` given a function definition and its
 * implementation.
 *
 * @public
 */
export declare function tool<const TParameters extends Record<string, {
    parse(input: any): any;
}>>({ name, description, parameters, implementation, }: {
    name: string;
    description: string;
    /**
     * The parameters of the function. Must be an with values being zod schemas.
     *
     * IMPORTANT
     *
     * The type here only requires an object with a `parse` function. This is not enough! We need an
     * actual zod schema because we will need to extract the JSON schema from it.
     *
     * The reason we only have a `parse` function here (as oppose to actually requiring ZodType is due
     * to this zod bug causing TypeScript breakage, when multiple versions of zod exist.
     *
     * - https://github.com/colinhacks/zod/issues/577
     * - https://github.com/colinhacks/zod/issues/2697
     * - https://github.com/colinhacks/zod/issues/3435
     */
    parameters: TParameters;
    implementation: (params: {
        [K in keyof TParameters]: TParameters[K] extends {
            parse: (input: any) => infer RReturnType;
        } ? RReturnType : never;
    }) => any | Promise<any>;
}): Tool;

/**
 * Shared properties of all tools.
 *
 * @public
 */
export declare interface ToolBase {
    name: string;
    description: string;
}

/**
 * @public
 */
export declare type ToolCallRequest = FunctionToolCallRequest;

/**
 * @public
 */
export declare type VirtualConfigSchematics = {
    [key: string]: {
        key: string;
        type: any;
        valueTypeKey: string;
    };
};

export { }
